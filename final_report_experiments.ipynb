{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b5fae5a-8007-4c60-a41f-ab02a3d9190e",
      "metadata": {
        "id": "7b5fae5a-8007-4c60-a41f-ab02a3d9190e"
      },
      "source": [
        "# Final Report Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6f85b9-0e99-4cf5-b21e-63de3d02eb04",
      "metadata": {
        "id": "ea6f85b9-0e99-4cf5-b21e-63de3d02eb04"
      },
      "source": [
        "This notebook contains the required modules for running experiments for the final report in Introduction to Deep Learning (11-785) course. The notebook is organized into the following sections:\n",
        "\n",
        "1.\n",
        "2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c43ba4-f6e6-4238-bb44-f56541f869d3",
      "metadata": {
        "id": "04c43ba4-f6e6-4238-bb44-f56541f869d3",
        "outputId": "1c731d09-1808-400b-a54e-f1fb1c123ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 26 04:24:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla V100-SXM2-32GB           On  |   00000000:16:00.0 Off |                    0 |\n",
            "| N/A   35C    P0             63W /  300W |       1MiB /  32768MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19c4d7e-bfc1-4097-a4b1-3173c03a9969",
      "metadata": {
        "id": "b19c4d7e-bfc1-4097-a4b1-3173c03a9969"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ceea003a-2ece-4572-bdf2-f04d84cac3a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceea003a-2ece-4572-bdf2-f04d84cac3a1",
        "outputId": "2efb92ca-ae86-40bb-f915-caf2c7951777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet # Install WandB\n",
        "!pip install pytorch_metric_learning --quiet # Install the Pytorch Metric Library\n",
        "!pip install torchinfo --quiet # Install torchinfo\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvLJF92HQ-yT",
        "outputId": "adb09a7e-2f6a-4965-be55-35432d492f7c"
      },
      "id": "zvLJF92HQ-yT",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f2405dc-bac2-4492-b17d-bc507e3559a6",
      "metadata": {
        "id": "5f2405dc-bac2-4492-b17d-bc507e3559a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchaudio\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_metric_learning import samplers\n",
        "from torchmetrics import F1Score\n",
        "\n",
        "#\n",
        "from transformers import Wav2Vec2Model\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bd62f225-98e4-47ee-9e2a-8f66d75c9ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd62f225-98e4-47ee-9e2a-8f66d75c9ad9",
        "outputId": "ce959d71-c47e-4c72-b1f1-bf0d38d50d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "!gdown https://drive.google.com/uc?id=1EZs76l2FYUJqEyCPqrKvTopee3TXnHxn\n",
        "!unzip -q task1_split.zip -d /content/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te6YslESNKI4",
        "outputId": "a52f4ac4-bd73-4e94-a536-80797dab621a"
      },
      "id": "te6YslESNKI4",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EZs76l2FYUJqEyCPqrKvTopee3TXnHxn\n",
            "From (redirected): https://drive.google.com/uc?id=1EZs76l2FYUJqEyCPqrKvTopee3TXnHxn&confirm=t&uuid=44d4f323-a81d-496b-b1c8-4f6718e764ae\n",
            "To: /content/task1_split.zip\n",
            "100% 414M/414M [00:07<00:00, 54.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f69a865-7c9d-4e76-b3b2-b450176fbb79",
      "metadata": {
        "id": "4f69a865-7c9d-4e76-b3b2-b450176fbb79"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c50d7cf2-0238-429a-bba3-16f4601276a8",
      "metadata": {
        "id": "c50d7cf2-0238-429a-bba3-16f4601276a8"
      },
      "outputs": [],
      "source": [
        "WAV2VEC_MODEL = \"facebook/wav2vec2-base-960h\"\n",
        "CACHE_DIR = \"\" # ADD CACHE DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "36f9312d-1a35-4225-9e4b-e86d71499549",
      "metadata": {
        "id": "36f9312d-1a35-4225-9e4b-e86d71499549"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # ----- Problem Configs -----\n",
        "    \"subset\" : 1,\n",
        "    \"task_num\" : 1,\n",
        "    \"fs\" : 8000, # in Hz\n",
        "    \"max_len\" : 5, # in seconds\n",
        "\n",
        "    # ----- Data Configs -----\n",
        "    \"train_dir\" : \"/content/data/task1_split/train\", # ADD DIR\n",
        "    \"val_dir\" :  \"/content/data/task1_split/val\", # ADD DIR\n",
        "    \"test_dir\" :  \"/content/data/task1_split/test\", # ADD DIR\n",
        "    \"metadata_path\": \"/content/data/task1_split/sand_task_1.xlsx\",\n",
        "\n",
        "    ## ----- Model Configs -----\n",
        "    \"feature_extractor\": \"wav2vec_lora\",   # wav2vec_lora or wav2vec_vanilla\n",
        "    \"post_processor\": \"none\",              # lstm or none\n",
        "    # shared dim for all modes\n",
        "    \"feature_dim\": 768,\n",
        "\n",
        "    # ---- attention pooling ----\n",
        "    # used only when mode == \"attention\"\n",
        "    \"attention_dim\": 768,\n",
        "\n",
        "    # ---- conv pooling ----\n",
        "    # used only when mode == \"conv\"\n",
        "    \"conv_kernel_size\": 5,\n",
        "    \"conv_padding\": 2,\n",
        "\n",
        "    # ---- multihead pooling ----\n",
        "    # used only when mode == \"multihead\"\n",
        "    \"multihead_num_heads\": 4,\n",
        "    \"classifier\": \"mlp\",                   # or linear\n",
        "\n",
        "    \"hidden_dim\": 768,\n",
        "    \"num_layers\": 2,\n",
        "    \"num_classes\": 6,\n",
        "    \"lora_r\": 8,\n",
        "    \"lora_alpha\": 16,\n",
        "\n",
        "    # ----- Training Configs -----\n",
        "    'batch_size': 3, # Increase this if your GPU can handle it\n",
        "    'lr': 1e-6,\n",
        "    \"weight_decay\" : 1e-4,\n",
        "    'epochs': 50,\n",
        "    'num_classes': 5,\n",
        "    'checkpoint_dir': \"./checkpoints\",\n",
        "    'augument': True,\n",
        "    'ablation_ID': 22\n",
        "}\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b292af74-9da9-4d4d-94a4-c28952e7cfcb",
      "metadata": {
        "id": "b292af74-9da9-4d4d-94a4-c28952e7cfcb"
      },
      "outputs": [],
      "source": [
        "phonation_list = [\"phonationA\",\n",
        "                  \"phonationE\",\n",
        "                  \"phonationI\",\n",
        "                  \"phonationO\",\n",
        "                  \"phonationU\",\n",
        "                  \"rhythmKA\",\n",
        "                  \"rhythmPA\",\n",
        "                  \"rhythmTA\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de019b1-d720-4123-83f0-9a6b772efe03",
      "metadata": {
        "id": "7de019b1-d720-4123-83f0-9a6b772efe03"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e82d876d-cd29-4d36-b5ed-e6c455c7876e",
      "metadata": {
        "id": "e82d876d-cd29-4d36-b5ed-e6c455c7876e"
      },
      "outputs": [],
      "source": [
        "class Wav2VecAugment:\n",
        "    def __init__(self,\n",
        "                p_noise=0.3,\n",
        "                p_gain=0.3,\n",
        "                p_flip=0.1,\n",
        "                p_shift=0.3,\n",
        "                p_filter=0.3,\n",
        "                p_speed=0.2,\n",
        "                sample_rate=config[\"fs\"]):\n",
        "        self.p_noise = p_noise\n",
        "        self.p_gain = p_gain\n",
        "        self.p_flip = p_flip # Flip polarity (not implemented)\n",
        "        self.p_shift = p_shift # Time shift (not implemented)\n",
        "        self.p_filter = p_filter # Simple HPF (not implemented)\n",
        "        self.p_speed = p_speed\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "\n",
        "    def __call__(self, waveform):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            waveform :\n",
        "                - Input wavefrom\n",
        "                - Type: tesnsor\n",
        "                - Shape : [T]\n",
        "        \"\"\"\n",
        "        # ----- Gaussian Noise -----\n",
        "        if random.random() < self.p_noise:\n",
        "            noise_amp = (0.01 * random.random()) * torch.rand_like(waveform)\n",
        "            waveform = waveform + noise_amp\n",
        "\n",
        "        # ----- Volume Augmentation -----\n",
        "        if random.random() < self.p_gain:\n",
        "            gain_db = random.uniform(-6, 6)\n",
        "            gain = 10**(gain_db/20)\n",
        "            waveform = waveform*gain\n",
        "\n",
        "        # ----- Speed Change -----\n",
        "        if random.random() < self.p_speed:\n",
        "            speed_factor = random.uniform(0.8, 1.2)\n",
        "            waveform = self.time_stretch(waveform, speed_factor)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "    def time_stretch(self, waveform, speed_factor):\n",
        "        new_sample_rate = int(self.sample_rate * speed_factor)\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=new_sample_rate)\n",
        "        return resampler(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12110973-fa6b-4516-b8f4-d2fb79e0904d",
      "metadata": {
        "id": "12110973-fa6b-4516-b8f4-d2fb79e0904d"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "68a4f142-6cf5-44b8-bcdd-e1d7e83777ae",
      "metadata": {
        "id": "68a4f142-6cf5-44b8-bcdd-e1d7e83777ae"
      },
      "outputs": [],
      "source": [
        "class Wav2VecAudioMultiPhonationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 root_dir,\n",
        "                 phonation_types,\n",
        "                 set_type,\n",
        "                 original_fs=config[\"fs\"],\n",
        "                 new_fs=16_000,\n",
        "                 metadata_file=\"sand_task_1.xlsx\",\n",
        "                 metadata_path=None,\n",
        "                 augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.phonation_types = phonation_types\n",
        "        self.set_type = set_type\n",
        "        self.augment = augment\n",
        "        self.original_fs = original_fs\n",
        "        self.new_fs = new_fs\n",
        "\n",
        "        # ----- Data Augmentaion -----\n",
        "        self.transform = Wav2VecAugment() if augment else None\n",
        "\n",
        "        # ----- Load Meta Data -----\n",
        "        if metadata_path is None:\n",
        "          metadata_path = os.path.join(root_dir, metadata_file)\n",
        "          self.metadata_df = pd.read_excel(metadata_path)\n",
        "        else:\n",
        "          self.metadata_df = pd.read_excel(metadata_path)\n",
        "\n",
        "        if (self.set_type != \"test\"):\n",
        "            self.id_to_label = dict(zip( self.metadata_df[\"ID\"], self.metadata_df[\"Class\"] ))\n",
        "        else:\n",
        "            self.id_to_label = None\n",
        "\n",
        "        # ----- Identify Subjects -----\n",
        "        self.subject_ids = self._collect_valid_subject_ids()\n",
        "\n",
        "        # ----- Resampling ----- (not needed, functional is used, robust to multi-freq recordings)\n",
        "        # self.resampler = torchaudio.transforms.Resample(orig_freq=original_fs,\n",
        "                                                       # new_freq=new_fs)\n",
        "\n",
        "    def _collect_valid_subject_ids(self):\n",
        "        phonation_files = {}\n",
        "        for phonation in self.phonation_types:\n",
        "            phonation_path = os.path.join(self.root_dir, phonation)\n",
        "            files = os.listdir(phonation_path)\n",
        "            files = os.listdir(phonation_path)\n",
        "            ids = set(f.split('_')[0] for f in files if f.endswith('.wav'))\n",
        "            phonation_files[phonation] = ids\n",
        "\n",
        "        valid_ids = set.intersection(*phonation_files.values())\n",
        "        valid_ids = valid_ids.intersection(\n",
        "            set(str(id_) for id_ in self.metadata_df['ID'])\n",
        "        )\n",
        "\n",
        "        return sorted(valid_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Input(s):\n",
        "            idx:\n",
        "                - sample index (0-based)\n",
        "                - type: int\n",
        "                - shape: (1,)\n",
        "\n",
        "        Output(s):\n",
        "            recordings:\n",
        "                - Recordings of different phonations from the same person.\n",
        "                - type: list\n",
        "                - shape: (num_phonations, ?). The second dimension is \"?\" because each recording has a different number of samples.\n",
        "            label:\n",
        "                - sample class label (0-based). There are 5 classes in total, so label in [0, 4].\n",
        "                - type: int\n",
        "                - shape: (1,)\n",
        "        \"\"\"\n",
        "        subject_id = self.subject_ids[idx]\n",
        "        recordings = []\n",
        "\n",
        "        for phonation in self.phonation_types:\n",
        "            # ----- Read audio file -----\n",
        "            filename = f\"{subject_id}_{phonation}.wav\"\n",
        "            filepath = os.path.join(self.root_dir, phonation, filename)\n",
        "            waveform, sample_rate = torchaudio.load(filepath) # [num_mics, T]\n",
        "            waveform = waveform.mean(dim=0) # [T]\n",
        "\n",
        "            # ----- Resample to 16kHz -----\n",
        "            if sample_rate != 16_000:\n",
        "                waveform = torchaudio.functional.resample(waveform,\n",
        "                                                         sample_rate,\n",
        "                                                         self.new_fs)\n",
        "\n",
        "            # ----- Apply augmentations -----\n",
        "            if self.transform:\n",
        "                waveform = self.transform(waveform)\n",
        "\n",
        "            recordings.append(waveform)\n",
        "\n",
        "        if (self.set_type == \"test\"):\n",
        "            return recordings\n",
        "        else:\n",
        "            label = int(self.id_to_label[subject_id] - 1) # 0-based classes\n",
        "            return recordings, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "29d58b8d-d7cc-4f51-9349-9ff0f70b8778",
      "metadata": {
        "id": "29d58b8d-d7cc-4f51-9349-9ff0f70b8778"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "# subset function\n",
        "def subset_function(dataset, subset_selection):\n",
        "\n",
        "    # only use subset if its less than 1\n",
        "    if subset_selection < 1.0:\n",
        "\n",
        "        subset_size = int(len(dataset) * subset_selection)\n",
        "        indices = random.sample(range(len(dataset)), subset_size)\n",
        "        print(f\"Using subset of {subset_size}/{len(dataset)} samples ({subset_selection*100:.0f}%)\")\n",
        "        return Subset(dataset, indices)\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(f\"Using full dataset ({len(dataset)} samples)\")\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4d5fe855-9c95-4605-9ef1-0ed2e6188785",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5fe855-9c95-4605-9ef1-0ed2e6188785",
        "outputId": "f5a05ec6-8263-4b94-cef0-8fb75f572e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using full dataset (190 samples)\n",
            "Using full dataset (40 samples)\n",
            "Using full dataset (40 samples)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"train_dir\"],\n",
        "                                                  phonation_types=phonation_list,\n",
        "                                                  set_type=\"train\",\n",
        "                                                  original_fs=config[\"fs\"],\n",
        "                                                  new_fs=16_000,\n",
        "                                                  metadata_file=\"sand_task_1.xlsx\",\n",
        "                                                  metadata_path=config[\"metadata_path\"],\n",
        "                                                  augment=True)\n",
        "train_dataset = subset_function(train_dataset,\n",
        "                                config['subset'])\n",
        "\n",
        "val_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"val_dir\"],\n",
        "                                                phonation_types=phonation_list,\n",
        "                                                set_type=\"val\",\n",
        "                                                original_fs=config[\"fs\"],\n",
        "                                                new_fs=16_000,\n",
        "                                                metadata_file=\"sand_task_1.xlsx\",\n",
        "                                                metadata_path=config[\"metadata_path\"],\n",
        "                                                augment=False)\n",
        "val_dataset = subset_function(val_dataset,\n",
        "                              config['subset'])\n",
        "\n",
        "test_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"val_dir\"],\n",
        "                                          phonation_types=phonation_list,\n",
        "                                          set_type=\"test\",\n",
        "                                          original_fs=config[\"fs\"],\n",
        "                                          new_fs=16_000,\n",
        "                                          metadata_file=\"sand_task_1.xlsx\",\n",
        "                                          metadata_path=config[\"metadata_path\"],\n",
        "                                          augment=False)\n",
        "\n",
        "test_dataset = subset_function(test_dataset,\n",
        "                               config['subset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5628d0ae-6d2a-4976-8947-9d8c3d1f5ce3",
      "metadata": {
        "id": "5628d0ae-6d2a-4976-8947-9d8c3d1f5ce3"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "94702df8-3bed-45b1-88e8-7a1370e084ed",
      "metadata": {
        "id": "94702df8-3bed-45b1-88e8-7a1370e084ed"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Input(s):\n",
        "        batch: list of tuples (audio_tensors, label)\n",
        "            - audio_tensors: [num_recordings, T], where T is not fixed for r in R.\n",
        "    \"\"\"\n",
        "    audios_list, labels_list = zip(*batch)\n",
        "    B = len(audios_list)\n",
        "    R = len(audios_list[0])\n",
        "\n",
        "    # ----- max len per batch -----\n",
        "    max_len = max(aud.shape[-1] for subject in audios_list for aud in subject)\n",
        "\n",
        "    padded_audios = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for subject in audios_list: # subject : [R, T]\n",
        "        subject_padded = []\n",
        "        subject_mask = []\n",
        "\n",
        "        for aud in subject: # aud [T] or [1, T]\n",
        "            if aud.ndim == 2:\n",
        "                aud = aud.squeeze(0) # remove channel_dim if [1, T]\n",
        "            L = aud.shape[0]\n",
        "            pad_amount = max_len - L\n",
        "            if pad_amount > 0:\n",
        "                aud = torch.nn.functional.pad(aud, (0, pad_amount))\n",
        "                mask = torch.cat([torch.ones(L), torch.zeros(pad_amount)])\n",
        "            else:\n",
        "                mask = torch.ones(L)\n",
        "\n",
        "            subject_padded.append(aud) # [T_max]\n",
        "            subject_mask.append(mask) # [T_max]\n",
        "\n",
        "        padded_audios.append(torch.stack(subject_padded)) # [R, T_max]\n",
        "        attention_masks.append(torch.stack(subject_mask)) # [R, T_max]\n",
        "\n",
        "    # [B, R, T]\n",
        "    padded_audios = torch.stack(padded_audios)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    labels = torch.tensor(labels_list)\n",
        "\n",
        "    return padded_audios, attention_masks, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "282bddd9-60c2-4781-9744-adb8f62c2910",
      "metadata": {
        "id": "282bddd9-60c2-4781-9744-adb8f62c2910"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97737395-2082-4d36-b952-08afae53d6bf",
      "metadata": {
        "id": "97737395-2082-4d36-b952-08afae53d6bf"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "892e10b5-00e9-4497-9fce-dcd31d6cfaa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "892e10b5-00e9-4497-9fce-dcd31d6cfaa7",
        "outputId": "eec80612-70db-4381-db9e-177ed112c2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wavs shape: torch.Size([3, 8, 350720])\n",
            "attn_mask shape: torch.Size([3, 8, 350720])\n",
            "labels shape: torch.Size([3])\n",
            "labels: tensor([4, 3, 2])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, data in enumerate(val_loader):\n",
        "    wavs, attn_mask, labels = data\n",
        "\n",
        "    print(\"wavs shape:\", wavs.shape)          # [B, R, T]\n",
        "    print(\"attn_mask shape:\", attn_mask.shape)\n",
        "    print(\"labels shape:\", labels.shape)\n",
        "    print(\"labels:\", labels)\n",
        "\n",
        "    # Visualize the first audio example\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(wavs[0][0].numpy())\n",
        "    plt.title(\"Raw Audio Waveform\")\n",
        "    plt.xlabel(\"Time (samples)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.show()\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e845d2e-87be-44f5-bcae-5f9e3da343f3",
      "metadata": {
        "id": "5e845d2e-87be-44f5-bcae-5f9e3da343f3"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32c0781-c751-430c-a469-81c91b74f966",
      "metadata": {
        "id": "a32c0781-c751-430c-a469-81c91b74f966"
      },
      "source": [
        "### Wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8d97a99e-545e-4879-9514-69a917b87bb9",
      "metadata": {
        "id": "8d97a99e-545e-4879-9514-69a917b87bb9"
      },
      "outputs": [],
      "source": [
        "class Wav2VecVanilla(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(WAV2VEC_MODEL,\n",
        "                                                   output_hidden_states=False,\n",
        "                                                   cache_dir=CACHE_DIR)\n",
        "        self.output_dim = self.wav2vec.config.hidden_size\n",
        "\n",
        "    def _compute_feat_mask(self, attention_mask):\n",
        "        \"\"\"\n",
        "        Convert input attention mask [B, T_samples] into\n",
        "        downsampled feature mask [B, T_feat].\n",
        "        \"\"\"\n",
        "        device = attention_mask.device\n",
        "        B, T = attention_mask.shape\n",
        "\n",
        "        input_lengths = attention_mask.sum(dim=1)  # [B]\n",
        "\n",
        "        feat_lengths = self.wav2vec._get_feat_extract_output_lengths(input_lengths)\n",
        "\n",
        "        max_feat_len = self.wav2vec._get_feat_extract_output_lengths(\n",
        "            torch.tensor([T], device=device)\n",
        "        )[0]\n",
        "\n",
        "        feat_mask = torch.arange(max_feat_len, device=device)[None, :] < feat_lengths[:, None]\n",
        "        return feat_mask  # bool or uint8, shape [B, T_feat]\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        out = self.wav2vec(wavs, attention_mask=attention_mask)\n",
        "        hidden = out.last_hidden_state # [B, T_feat, 768]\n",
        "        feat_mask = self._compute_feat_mask(attention_mask)\n",
        "        return hidden, feat_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "93317f35-544d-48c1-a595-9bfb92936db4",
      "metadata": {
        "id": "93317f35-544d-48c1-a595-9bfb92936db4"
      },
      "outputs": [],
      "source": [
        "class Wav2VecLoRA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base = Wav2Vec2Model.from_pretrained(WAV2VEC_MODEL,\n",
        "                                           output_hidden_states=False,\n",
        "                                           cache_dir=CACHE_DIR)\n",
        "        self.output_dim = self.base.config.hidden_size\n",
        "\n",
        "        # ----- LoRA -----\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=config.get(\"lora_r\", 1),\n",
        "            lora_alpha=config.get(\"lora_alpha\", 16),\n",
        "            target_modules=config.get(\"target_modules\", [\"q_proj\", \"k_proj\", \"v_proj\"]), # can also do just [\"q_proj\"]\n",
        "            lora_dropout=0.0,\n",
        "            bias=\"none\"\n",
        "        )\n",
        "\n",
        "        self.wav2vec = get_peft_model(self.base, lora_cfg)\n",
        "        self.output_dime = self.base.config.hidden_size\n",
        "\n",
        "\n",
        "    def _compute_feat_mask(self, attention_mask):\n",
        "        \"\"\"\n",
        "        Convert input attention mask [B, T_samples] into\n",
        "        downsampled feature mask [B, T_feat].\n",
        "        \"\"\"\n",
        "        B, T = attention_mask.shape\n",
        "        device = attention_mask.device\n",
        "\n",
        "        input_lengths = attention_mask.sum(dim=1)  # [B]\n",
        "\n",
        "        feat_lengths = self.base._get_feat_extract_output_lengths(input_lengths)\n",
        "\n",
        "        # Max possible T_feat for this batch\n",
        "        max_feat_len = self.base._get_feat_extract_output_lengths(\n",
        "            torch.tensor([T], device=device)\n",
        "        )[0]\n",
        "\n",
        "        feat_mask = torch.arange(max_feat_len, device=device)[None, :] < feat_lengths[:, None]\n",
        "        return feat_mask  # [B, T_feat]\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        out = self.wav2vec(wavs, attention_mask=attention_mask)\n",
        "        hidden = out.last_hidden_state # [B, T_feat, 768]\n",
        "        feat_mask = self._compute_feat_mask(attention_mask)  # [B, T_feat]\n",
        "        return hidden, feat_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6d52c668-d28d-4d94-9640-e3393e4c1c61",
      "metadata": {
        "id": "6d52c668-d28d-4d94-9640-e3393e4c1c61"
      },
      "outputs": [],
      "source": [
        "class Wav2VecWrapper(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        fe = config[\"feature_extractor\"]\n",
        "        if fe == \"wav2vec_vanilla\":\n",
        "            self.feature_extractor = Wav2VecVanilla(config)\n",
        "        elif fe == \"wav2vec_lora\":\n",
        "            self.feature_extractor = Wav2VecLoRA(config)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown feature extractor: {fe}\")\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        B, R, T = wavs.shape\n",
        "        # assert R == self.R, f\"Expected R={self.R}, got {R}\"\n",
        "\n",
        "        # ----- Flatten batch*recordings for parallel Wav2Vec -----\n",
        "        wavs_flat = wavs.reshape(B*R, T)\n",
        "        mask_flat = attention_mask.reshape(B*R, T)\n",
        "\n",
        "        out, padding_mask = self.feature_extractor(wavs_flat, mask_flat)  # [B*R, T_feat, D], [B*R, T_feat]\n",
        "        out = out.reshape(B, R, -1, self.feature_extractor.output_dim) # Reshape to [B, R, T_feat]\n",
        "        padding_mask = padding_mask.reshape(B, R, -1) # Reshape to [B, R, T_feat]\n",
        "        assert out.shape[2] == padding_mask.shape[2], f\"Expected second dimension of out={out.shape} == padding_mask={padding_mask.shape}\"\n",
        "        return out, padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c178bd85-62f8-4ed4-9bf5-66eba42a7604",
      "metadata": {
        "id": "c178bd85-62f8-4ed4-9bf5-66eba42a7604"
      },
      "source": [
        "### Feature Extractor from Wav2Vec Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "895868e6-beea-46b5-9148-e1a9b3acbfed",
      "metadata": {
        "id": "895868e6-beea-46b5-9148-e1a9b3acbfed"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SeqFeatureExtractorWrapper(nn.Module):\n",
        "    def __init__(self, config, num_heads=4):\n",
        "        \"\"\"\n",
        "        Pools only across the time dimension T.\n",
        "        Input:  (B, T, D)\n",
        "        Output: (B, D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mode = config[\"post_processor\"]\n",
        "        if self.mode is None or self.mode == \"none\":\n",
        "          self.mode = \"last\"\n",
        "\n",
        "        elif self.mode == \"attention\":\n",
        "            self.att = nn.Linear(config[\"attention_dim\"], 1)\n",
        "\n",
        "        elif self.mode == \"conv\":\n",
        "            self.conv = nn.Conv1d(config[\"feature_dim\"], config[\"feature_dim\"], kernel_size=config[\"conv_kernel_size\"], padding=config[\"conv_padding\"])\n",
        "\n",
        "        elif self.mode == \"multihead\":\n",
        "            self.query = nn.Parameter(torch.randn(1, 1, config[\"attention_dim\"]))\n",
        "            self.att_mh = nn.MultiheadAttention(config[\"attention_dim\"], num_heads, batch_first=True)\n",
        "\n",
        "    def post_processor(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        mask: (B, T)   True = valid, False = pad\n",
        "        \"\"\"\n",
        "        # --------------------\n",
        "        # LAST-HIDDEN\n",
        "        # --------------------\n",
        "        if self.mode == \"last\":\n",
        "            idx = mask.float().flip(1).argmax(1)\n",
        "            idx[idx == 0] = mask.size(1) - 1\n",
        "            return x[torch.arange(x.size(0)), idx]\n",
        "\n",
        "        # --------------------\n",
        "        # MEAN\n",
        "        # --------------------\n",
        "        if self.mode == \"mean\":\n",
        "            x_masked = x * mask.unsqueeze(-1)\n",
        "            lengths = mask.sum(1).clamp(min=1).unsqueeze(-1)\n",
        "            return (x_masked.sum(1) / lengths)\n",
        "\n",
        "        # --------------------\n",
        "        # MAX\n",
        "        # --------------------\n",
        "        if self.mode == \"max\":\n",
        "            x_masked = x.masked_fill(~mask.unsqueeze(-1), -1e9)\n",
        "            return x_masked.max(1)[0]\n",
        "\n",
        "        # --------------------\n",
        "        # ATTENTION\n",
        "        # --------------------\n",
        "        if self.mode == \"attention\":\n",
        "            scores = self.att(x).squeeze(-1)\n",
        "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
        "            weights = torch.softmax(scores, 1).unsqueeze(-1)\n",
        "            return (weights * x).sum(1)\n",
        "\n",
        "        # --------------------\n",
        "        # CONV\n",
        "        # --------------------\n",
        "        if self.mode == \"conv\":\n",
        "            z = x.transpose(1, 2)          # (B, D, T)\n",
        "            z = torch.relu(self.conv(z))   # (B, D, T)\n",
        "            return z.mean(-1)              # (B, D)\n",
        "\n",
        "        # --------------------\n",
        "        # MULTIHEAD ATTENTION\n",
        "        # --------------------\n",
        "        if self.mode == \"multihead\":\n",
        "            B = x.size(0)\n",
        "            q = self.query.repeat(B, 1, 1)\n",
        "            pooled, _ = self.att_mh(q, x, x, key_padding_mask=~mask)\n",
        "            return pooled.squeeze(1)\n",
        "\n",
        "        raise ValueError(self.mode)\n",
        "    def forward(self, x, padding_mask):\n",
        "      B, R, T, D = x.shape\n",
        "      assert x.shape[:3] == padding_mask.shape, f\"Expected x.shape[0:3]={x.shape}==padding_mask.shape={padding_mask.shape}\"\n",
        "      x_flat = x.reshape(B*R, T, D)\n",
        "      mask_flat = padding_mask.reshape(B*R, T)\n",
        "      # ----- Eliminate Time Axis -----\n",
        "      out = self.post_processor(x_flat, mask_flat) # [B*R, D]\n",
        "      out = out.reshape(B, R, -1) # [B, R, D]\n",
        "\n",
        "      # ----- Fuse Multiple Recordings -----\n",
        "      out = out.mean(dim=1) # [B, D]\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92a4065-dfe5-4fc4-aa86-3b1687701199",
      "metadata": {
        "id": "e92a4065-dfe5-4fc4-aa86-3b1687701199"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "dfc3e140-fa80-490d-b870-52bf3fc835c8",
      "metadata": {
        "id": "dfc3e140-fa80-490d-b870-52bf3fc835c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        D = config.get(\"hidden_dim\", 768)\n",
        "        num_classes = config[\"num_classes\"]\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(D, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5346efd2-e66a-46eb-9174-2c047cd0f513",
      "metadata": {
        "id": "5346efd2-e66a-46eb-9174-2c047cd0f513"
      },
      "source": [
        "### ALS Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2fafecb1-8d84-42a6-a1ec-a94da1aaeddf",
      "metadata": {
        "id": "2fafecb1-8d84-42a6-a1ec-a94da1aaeddf"
      },
      "outputs": [],
      "source": [
        "class ALSDetectionModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings_extractor = Wav2VecWrapper(config)\n",
        "        self.sequence_postprocessor = SeqFeatureExtractorWrapper(config)\n",
        "        self.classifier = MLPClassifier(config)\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "\n",
        "        out, padding_mask = self.embeddings_extractor(wavs, attention_mask)\n",
        "\n",
        "        out = self.sequence_postprocessor(out, padding_mask)\n",
        "\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "4ff9324f-5bb0-4b5a-ad42-bdb9eff40c7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff9324f-5bb0-4b5a-ad42-bdb9eff40c7a",
        "outputId": "e9d8e487-2a77-4416-cd9e-174d68c38790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = ALSDetectionModel(config).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "1bd07c30-0413-4e93-a6b6-b27b6cdee5b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bd07c30-0413-4e93-a6b6-b27b6cdee5b0",
        "outputId": "3fc01ade-9390-4102-e84f-40d742e3ca77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 5])\n"
          ]
        }
      ],
      "source": [
        "B, P, T = 2, 8, 16000\n",
        "x = torch.randn(B, P, T).to(DEVICE)\n",
        "mask = torch.ones_like(x, dtype=torch.long)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(x, mask)\n",
        "\n",
        "print(\"Logits shape:\", out.shape)  # Expected: [B, num_classes] → [2, num_classes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d877dce5-c58e-4fe6-b7c8-c9dd71d6801a",
      "metadata": {
        "id": "d877dce5-c58e-4fe6-b7c8-c9dd71d6801a"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-7)# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63278aa-3947-43d7-b45c-afe3a925ed64",
      "metadata": {
        "id": "a63278aa-3947-43d7-b45c-afe3a925ed64"
      },
      "source": [
        "### Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b974c1f-66e9-4970-b1fc-185c2816aefc",
      "metadata": {
        "id": "4b974c1f-66e9-4970-b1fc-185c2816aefc"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, scaler):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0.0, 0.0\n",
        "    # batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (audio_tensors, attention_mask, labels) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        audio_tensors = audio_tensors.to(DEVICE)      # [B, R, T]\n",
        "        attention_mask = attention_mask.to(DEVICE)    # [B, R, T]\n",
        "        labels = labels.to(DEVICE)                    # [B]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
        "            logits = model(audio_tensors, attention_mask)  # [B, num_classes]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tloss += loss.item()\n",
        "        tacc += (torch.argmax(logits, dim=1) == labels).sum().item() / labels.size(0)\n",
        "\n",
        "        # batch_bar.set_postfix(\n",
        "        #     loss=\"{:.4f}\".format(tloss / (i + 1)),\n",
        "        #     acc=\"{:.4f}%\".format(tacc * 100 / (i + 1))\n",
        "        # )\n",
        "        # batch_bar.update()\n",
        "\n",
        "        del audio_tensors, attention_mask, labels, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # batch_bar.close()\n",
        "    tloss /= len(dataloader)\n",
        "    tacc /= len(dataloader)\n",
        "\n",
        "    return tloss, tacc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c5955f-bb40-40f5-86b6-38d0f80bbe7b",
      "metadata": {
        "id": "02c5955f-bb40-40f5-86b6-38d0f80bbe7b"
      },
      "outputs": [],
      "source": [
        "f1 = F1Score(num_classes=config[\"num_classes\"], average='macro', task='multiclass').cpu()\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    vloss, vacc = 0.0, 0.0\n",
        "    # batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (audio_tensors, attention_mask, labels) in enumerate(dataloader):\n",
        "        audio_tensors = audio_tensors.to(DEVICE)     # [B, R, T]\n",
        "        attention_mask = attention_mask.to(DEVICE)   # [B, R, T]\n",
        "        labels = labels.to(DEVICE)                   # [B]\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            logits = model(audio_tensors, attention_mask)  # [B, num_classes]\n",
        "            loss = criterion(logits, labels)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        vloss += loss.item()\n",
        "        vacc += (preds == labels).sum().item() / labels.size(0)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "\n",
        "        # batch_bar.set_postfix(\n",
        "        #     loss=\"{:.4f}\".format(vloss / (i + 1)),\n",
        "        #     acc=\"{:.4f}%\".format(vacc * 100 / (i + 1))\n",
        "        # )\n",
        "        # batch_bar.update()\n",
        "\n",
        "        del audio_tensors, attention_mask, labels, logits, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # batch_bar.close()\n",
        "\n",
        "    vloss /= len(dataloader)\n",
        "    vacc /= len(dataloader)\n",
        "\n",
        "    # concatenate all predictions and labels\n",
        "    all_preds = torch.cat(all_preds).cpu()\n",
        "    all_labels = torch.cat(all_labels).cpu()\n",
        "\n",
        "    vf1 = f1(all_preds.cpu(), all_labels.cpu()).cpu()  # make sure f1 accepts CPU tensors\n",
        "\n",
        "    del all_preds, all_labels\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return vloss, vacc, vf1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada5b91b-b018-4e96-85cb-51fa4e1d7354",
      "metadata": {
        "id": "ada5b91b-b018-4e96-85cb-51fa4e1d7354"
      },
      "source": [
        "### Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3d24fc-6a2e-4534-ad48-4cefa3feb276",
      "metadata": {
        "id": "5f3d24fc-6a2e-4534-ad48-4cefa3feb276",
        "outputId": "5a2de009-d11e-441f-ab22-87c52b2e2e0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibrahim-m-aldarmaki\u001b[0m (\u001b[33mibrahim-m-aldarmaki-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add api key\n",
        "wandb.login(key=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4409c277-27f9-4e45-8c33-262c1ea07461",
      "metadata": {
        "id": "4409c277-27f9-4e45-8c33-262c1ea07461",
        "outputId": "783138e5-2882-4f15-8d15-983f80b223c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing new WanDB run...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/ocean/projects/cis220031p/aldarmak/IDL_Project/final/wandb/run-20251126_042701-8fp9h0ry</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized/runs/8fp9h0ry' target=\"_blank\">ablation 22</a></strong> to <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized/runs/8fp9h0ry' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized/runs/8fp9h0ry</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "RESUME_OLD_RUN = False\n",
        "\n",
        "if RESUME_OLD_RUN == True:\n",
        "    print(\"Resuming previous WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        #id     = None,\n",
        "        resume = \"must\",\n",
        "        project = \"SAND\",\n",
        "        config  = config\n",
        "    )\n",
        "else:\n",
        "    print(\"Initializing new WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        reinit  = True,\n",
        "        project = None,\n",
        "        config  = config\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb967bd8-9a8b-49c0-88ca-c356a7848107",
      "metadata": {
        "id": "bb967bd8-9a8b-49c0-88ca-c356a7848107",
        "outputId": "015c4116-c5d1-4332-a310-232412f36f9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ablation 22</strong> at: <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized/runs/8fp9h0ry' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized/runs/8fp9h0ry</a><br> View project at: <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251126_042701-8fp9h0ry/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/ocean/projects/cis220031p/aldarmak/IDL_Project/final/wandb/run-20251126_042702-s62d9vlj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project/runs/s62d9vlj' target=\"_blank\">last_run</a></strong> to <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project/runs/s62d9vlj' target=\"_blank\">https://wandb.ai/ibrahim-m-aldarmaki-carnegie-mellon-university/my_project/runs/s62d9vlj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "\tTrain Acc 0.0000%\tTrain Loss nan\tLR 0.0000010\n",
            "\tVal Acc 33.3333%\tVal Loss 1.5949\tF1 0.1429\n",
            "Saved new best model with Val F1: 0.1428571492433548 at last_run/best_model_epoch_1.pth\n",
            "\n",
            "Epoch 2/50\n",
            "\tTrain Acc 0.0000%\tTrain Loss nan\tLR 0.0000010\n",
            "\tVal Acc 33.3333%\tVal Loss 1.5949\tF1 0.1429\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_val_f1 = 0.0\n",
        "run_name = 'last_run'\n",
        "os.makedirs(run_name, exist_ok=True)\n",
        "\n",
        "wandb.init(project=\"my_project\", name=run_name)\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, scaler)\n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"\\tTrain Acc {train_acc*100:.04f}%\\tTrain Loss {train_loss:.04f}\\tLR {curr_lr:.7f}\")\n",
        "    print(f\"\\tVal Acc {val_acc*100:.04f}%\\tVal Loss {val_loss:.04f}\\tF1 {val_f1:.04f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "        'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr, 'valid_F1': val_f1\n",
        "    })\n",
        "\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_f1': best_val_f1,\n",
        "            'train_acc': train_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        checkpoint_path = os.path.join(run_name, f'best_model_epoch_{epoch+1}.pth')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Saved new best model with Val F1: {val_f1} at {checkpoint_path}\")\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd877244-f369-4445-8a50-4cbb11005e80",
      "metadata": {
        "id": "dd877244-f369-4445-8a50-4cbb11005e80"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd650601-8aa3-41d0-8e2a-f4ceb74847b1",
      "metadata": {
        "id": "fd650601-8aa3-41d0-8e2a-f4ceb74847b1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}