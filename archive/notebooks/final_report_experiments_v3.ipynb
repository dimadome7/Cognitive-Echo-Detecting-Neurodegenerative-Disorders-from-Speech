{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b5fae5a-8007-4c60-a41f-ab02a3d9190e",
      "metadata": {
        "id": "7b5fae5a-8007-4c60-a41f-ab02a3d9190e"
      },
      "source": [
        "# Final Report Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6f85b9-0e99-4cf5-b21e-63de3d02eb04",
      "metadata": {
        "id": "ea6f85b9-0e99-4cf5-b21e-63de3d02eb04"
      },
      "source": [
        "This notebook contains the required modules for running experiments for the final report in Introduction to Deep Learning (11-785) course. The notebook is organized into the following sections:\n",
        "\n",
        "1.\n",
        "2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c43ba4-f6e6-4238-bb44-f56541f869d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04c43ba4-f6e6-4238-bb44-f56541f869d3",
        "outputId": "bf6dcc25-724b-46c7-d25e-1a298d8f2a28"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19c4d7e-bfc1-4097-a4b1-3173c03a9969",
      "metadata": {
        "id": "b19c4d7e-bfc1-4097-a4b1-3173c03a9969"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceea003a-2ece-4572-bdf2-f04d84cac3a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceea003a-2ece-4572-bdf2-f04d84cac3a1",
        "outputId": "451c8975-b053-4378-8a3b-342b3e539507"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet # Install WandB\n",
        "!pip install pytorch_metric_learning --quiet # Install the Pytorch Metric Library\n",
        "!pip install torchinfo --quiet # Install torchinfo\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zvLJF92HQ-yT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvLJF92HQ-yT",
        "outputId": "b24f7367-2bb7-49e8-a568-309a07b6e085"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2405dc-bac2-4492-b17d-bc507e3559a6",
      "metadata": {
        "id": "5f2405dc-bac2-4492-b17d-bc507e3559a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchaudio\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as T\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_metric_learning import samplers\n",
        "from torchmetrics import F1Score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "#\n",
        "from transformers import Wav2Vec2Model\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd62f225-98e4-47ee-9e2a-8f66d75c9ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd62f225-98e4-47ee-9e2a-8f66d75c9ad9",
        "outputId": "f5f8b2ba-32a5-401a-a258-131ccdf2339d"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "te6YslESNKI4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te6YslESNKI4",
        "outputId": "c9ecc4f3-ac67-47ea-a20a-7fa4dd17c574"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/data\n",
        "!gdown https://drive.google.com/uc?id=1EZs76l2FYUJqEyCPqrKvTopee3TXnHxn\n",
        "!unzip -q task1_split.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cza_WJsYBikC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cza_WJsYBikC",
        "outputId": "2b3aaf5e-86ed-43d1-dd8c-bc2967aae792"
      },
      "outputs": [],
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f69a865-7c9d-4e76-b3b2-b450176fbb79",
      "metadata": {
        "id": "4f69a865-7c9d-4e76-b3b2-b450176fbb79"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50d7cf2-0238-429a-bba3-16f4601276a8",
      "metadata": {
        "id": "c50d7cf2-0238-429a-bba3-16f4601276a8"
      },
      "outputs": [],
      "source": [
        "WAV2VEC_MODEL = \"facebook/wav2vec2-base-960h\"\n",
        "CACHE_DIR = os.makedirs('/content/cache', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f9312d-1a35-4225-9e4b-e86d71499549",
      "metadata": {
        "id": "36f9312d-1a35-4225-9e4b-e86d71499549"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # ----- Problem Configs -----\n",
        "    \"subset\" : 1,\n",
        "    \"task_num\" : 1,\n",
        "    \"fs\" : 8000, # in Hz\n",
        "    \"max_len\" : 5, # in seconds\n",
        "\n",
        "    # ----- Data Configs -----\n",
        "    \"train_dir\" : \"/content/data/task1_split/train\", # ADD DIR\n",
        "    \"val_dir\" :  \"/content/data/task1_split/val\", # ADD DIR\n",
        "    \"test_dir\" :  \"/content/data/task1_split/test\", # ADD DIR\n",
        "    \"metadata_path\": \"/content/data/task1_split/sand_task_1.xlsx\",\n",
        "\n",
        "    ## ----- Model Configs -----\n",
        "    \"feature_extractor\": \"wav2vec_lora\",   # wav2vec_lora or wav2vec_vanilla\n",
        "    \"post_processor\": \"attention\",              # lstm or none\n",
        "    # shared dim for all modes\n",
        "    \"feature_dim\": 768,\n",
        "\n",
        "    # ---- attention pooling ----\n",
        "    # used only when mode == \"attention\"\n",
        "    \"attention_dim\": 768,\n",
        "\n",
        "    # ---- conv pooling ----\n",
        "    # used only when mode == \"conv\"\n",
        "    \"conv_kernel_size\": 5,\n",
        "    \"conv_padding\": 2,\n",
        "\n",
        "    # ---- multihead pooling ----\n",
        "    # used only when mode == \"multihead\"\n",
        "    \"multihead_num_heads\": 4,\n",
        "    \"classifier\": \"mlp\",                   # or linear\n",
        "\n",
        "    \"hidden_dim\": 768,\n",
        "    \"num_layers\": 2,\n",
        "    \"num_classes\": 5,\n",
        "    \"lora_r\": 8,\n",
        "    \"lora_alpha\": 16,\n",
        "\n",
        "    # ----- Training Configs -----\n",
        "    'batch_size': 3, # Increase this if your GPU can handle it\n",
        "    'lr': 3e-4,\n",
        "    \"weight_decay\" : 1e-4,\n",
        "    'epochs': 50,\n",
        "    'num_classes': 5,\n",
        "    'checkpoint_dir': \"./checkpoints\",\n",
        "    'augument': True,\n",
        "    'ablation_ID': 33\n",
        "}\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b292af74-9da9-4d4d-94a4-c28952e7cfcb",
      "metadata": {
        "id": "b292af74-9da9-4d4d-94a4-c28952e7cfcb"
      },
      "outputs": [],
      "source": [
        "phonation_list = [\"phonationA\",\n",
        "                  \"phonationE\",\n",
        "                  \"phonationI\",\n",
        "                  \"phonationO\",\n",
        "                  \"phonationU\",\n",
        "                  \"rhythmKA\",\n",
        "                  \"rhythmPA\",\n",
        "                  \"rhythmTA\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de019b1-d720-4123-83f0-9a6b772efe03",
      "metadata": {
        "id": "7de019b1-d720-4123-83f0-9a6b772efe03"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82d876d-cd29-4d36-b5ed-e6c455c7876e",
      "metadata": {
        "id": "e82d876d-cd29-4d36-b5ed-e6c455c7876e"
      },
      "outputs": [],
      "source": [
        "class Wav2VecAugment:\n",
        "    def __init__(self,\n",
        "                p_noise=0.3,\n",
        "                p_gain=0.3,\n",
        "                p_flip=0.1,\n",
        "                p_shift=0.3,\n",
        "                p_filter=0.3,\n",
        "                p_speed=0.2,\n",
        "                sample_rate=config[\"fs\"]):\n",
        "        self.p_noise = p_noise\n",
        "        self.p_gain = p_gain\n",
        "        self.p_flip = p_flip # Flip polarity (not implemented)\n",
        "        self.p_shift = p_shift # Time shift (not implemented)\n",
        "        self.p_filter = p_filter # Simple HPF (not implemented)\n",
        "        self.p_speed = p_speed\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "\n",
        "    def __call__(self, waveform):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            waveform :\n",
        "                - Input wavefrom\n",
        "                - Type: tesnsor\n",
        "                - Shape : [T]\n",
        "        \"\"\"\n",
        "        # ----- Gaussian Noise -----\n",
        "        if random.random() < self.p_noise:\n",
        "            noise_amp = (0.01 * random.random()) * torch.rand_like(waveform)\n",
        "            waveform = waveform + noise_amp\n",
        "\n",
        "        # ----- Volume Augmentation -----\n",
        "        if random.random() < self.p_gain:\n",
        "            gain_db = random.uniform(-6, 6)\n",
        "            gain = 10**(gain_db/20)\n",
        "            waveform = waveform*gain\n",
        "\n",
        "        # ----- Speed Change -----\n",
        "        if random.random() < self.p_speed:\n",
        "            speed_factor = random.uniform(0.8, 1.2)\n",
        "            waveform = self.time_stretch(waveform, speed_factor)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "    def time_stretch(self, waveform, speed_factor):\n",
        "        new_sample_rate = int(self.sample_rate * speed_factor)\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=new_sample_rate)\n",
        "        return resampler(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12110973-fa6b-4516-b8f4-d2fb79e0904d",
      "metadata": {
        "id": "12110973-fa6b-4516-b8f4-d2fb79e0904d"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a4f142-6cf5-44b8-bcdd-e1d7e83777ae",
      "metadata": {
        "id": "68a4f142-6cf5-44b8-bcdd-e1d7e83777ae"
      },
      "outputs": [],
      "source": [
        "class Wav2VecAudioMultiPhonationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 root_dir,\n",
        "                 phonation_types,\n",
        "                 set_type,\n",
        "                 original_fs=config[\"fs\"],\n",
        "                 new_fs=16_000,\n",
        "                 metadata_file=\"sand_task_1.xlsx\",\n",
        "                 metadata_path=None,\n",
        "                 augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.phonation_types = phonation_types\n",
        "        self.set_type = set_type\n",
        "        self.augment = augment\n",
        "        self.original_fs = original_fs\n",
        "        self.new_fs = new_fs\n",
        "\n",
        "        # ----- Data Augmentaion -----\n",
        "        self.transform = Wav2VecAugment() if augment else None\n",
        "\n",
        "        # ----- Load Meta Data -----\n",
        "        if metadata_path is None:\n",
        "          metadata_path = os.path.join(root_dir, metadata_file)\n",
        "          self.metadata_df = pd.read_excel(metadata_path)\n",
        "        else:\n",
        "          self.metadata_df = pd.read_excel(metadata_path)\n",
        "\n",
        "        if (self.set_type != \"test\"):\n",
        "            self.id_to_label = dict(zip( self.metadata_df[\"ID\"], self.metadata_df[\"Class\"] ))\n",
        "        else:\n",
        "            self.id_to_label = None\n",
        "\n",
        "        # ----- Identify Subjects -----\n",
        "        self.subject_ids = self._collect_valid_subject_ids()\n",
        "\n",
        "        # ----- Resampling ----- (not needed, functional is used, robust to multi-freq recordings)\n",
        "        # self.resampler = torchaudio.transforms.Resample(orig_freq=original_fs,\n",
        "                                                       # new_freq=new_fs)\n",
        "\n",
        "    def _collect_valid_subject_ids(self):\n",
        "        phonation_files = {}\n",
        "        for phonation in self.phonation_types:\n",
        "            phonation_path = os.path.join(self.root_dir, phonation)\n",
        "            files = os.listdir(phonation_path)\n",
        "            files = os.listdir(phonation_path)\n",
        "            ids = set(f.split('_')[0] for f in files if f.endswith('.wav'))\n",
        "            phonation_files[phonation] = ids\n",
        "\n",
        "        valid_ids = set.intersection(*phonation_files.values())\n",
        "        valid_ids = valid_ids.intersection(\n",
        "            set(str(id_) for id_ in self.metadata_df['ID'])\n",
        "        )\n",
        "\n",
        "        return sorted(valid_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Input(s):\n",
        "            idx:\n",
        "                - sample index (0-based)\n",
        "                - type: int\n",
        "                - shape: (1,)\n",
        "\n",
        "        Output(s):\n",
        "            recordings:\n",
        "                - Recordings of different phonations from the same person.\n",
        "                - type: list\n",
        "                - shape: (num_phonations, ?). The second dimension is \"?\" because each recording has a different number of samples.\n",
        "            label:\n",
        "                - sample class label (0-based). There are 5 classes in total, so label in [0, 4].\n",
        "                - type: int\n",
        "                - shape: (1,)\n",
        "        \"\"\"\n",
        "        subject_id = self.subject_ids[idx]\n",
        "        recordings = []\n",
        "\n",
        "        for phonation in self.phonation_types:\n",
        "            # ----- Read audio file -----\n",
        "            filename = f\"{subject_id}_{phonation}.wav\"\n",
        "            filepath = os.path.join(self.root_dir, phonation, filename)\n",
        "            waveform, sample_rate = torchaudio.load(filepath) # [num_mics, T]\n",
        "            waveform = waveform.mean(dim=0) # [T]\n",
        "\n",
        "            # ----- Resample to 16kHz -----\n",
        "            if sample_rate != 16_000:\n",
        "                waveform = torchaudio.functional.resample(waveform,\n",
        "                                                         sample_rate,\n",
        "                                                         self.new_fs)\n",
        "\n",
        "            # ----- Apply augmentations -----\n",
        "            if self.transform:\n",
        "                waveform = self.transform(waveform)\n",
        "\n",
        "            recordings.append(waveform)\n",
        "\n",
        "        if (self.set_type == \"test\"):\n",
        "            return recordings\n",
        "        else:\n",
        "            label = int(self.id_to_label[subject_id] - 1) # 0-based classes\n",
        "            return recordings, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d58b8d-d7cc-4f51-9349-9ff0f70b8778",
      "metadata": {
        "id": "29d58b8d-d7cc-4f51-9349-9ff0f70b8778"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "# subset function\n",
        "def subset_function(dataset, subset_selection):\n",
        "\n",
        "    # only use subset if its less than 1\n",
        "    if subset_selection < 1.0:\n",
        "\n",
        "        subset_size = int(len(dataset) * subset_selection)\n",
        "        indices = random.sample(range(len(dataset)), subset_size)\n",
        "        print(f\"Using subset of {subset_size}/{len(dataset)} samples ({subset_selection*100:.0f}%)\")\n",
        "        return Subset(dataset, indices)\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(f\"Using full dataset ({len(dataset)} samples)\")\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5fe855-9c95-4605-9ef1-0ed2e6188785",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5fe855-9c95-4605-9ef1-0ed2e6188785",
        "outputId": "7aa37cb1-e8a1-4bee-8e32-8eab6cdbf87d"
      },
      "outputs": [],
      "source": [
        "train_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"train_dir\"],\n",
        "                                                  phonation_types=phonation_list,\n",
        "                                                  set_type=\"train\",\n",
        "                                                  original_fs=config[\"fs\"],\n",
        "                                                  new_fs=16_000,\n",
        "                                                  metadata_file=\"sand_task_1.xlsx\",\n",
        "                                                  metadata_path=config[\"metadata_path\"],\n",
        "                                                  augment=True)\n",
        "train_dataset = subset_function(train_dataset,\n",
        "                                config['subset'])\n",
        "\n",
        "val_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"val_dir\"],\n",
        "                                                phonation_types=phonation_list,\n",
        "                                                set_type=\"val\",\n",
        "                                                original_fs=config[\"fs\"],\n",
        "                                                new_fs=16_000,\n",
        "                                                metadata_file=\"sand_task_1.xlsx\",\n",
        "                                                metadata_path=config[\"metadata_path\"],\n",
        "                                                augment=False)\n",
        "val_dataset = subset_function(val_dataset,\n",
        "                              config['subset'])\n",
        "\n",
        "test_dataset = Wav2VecAudioMultiPhonationDataset(root_dir=config[\"val_dir\"],\n",
        "                                          phonation_types=phonation_list,\n",
        "                                          set_type=\"test\",\n",
        "                                          original_fs=config[\"fs\"],\n",
        "                                          new_fs=16_000,\n",
        "                                          metadata_file=\"sand_task_1.xlsx\",\n",
        "                                          metadata_path=config[\"metadata_path\"],\n",
        "                                          augment=False)\n",
        "\n",
        "test_dataset = subset_function(test_dataset,\n",
        "                               config['subset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5628d0ae-6d2a-4976-8947-9d8c3d1f5ce3",
      "metadata": {
        "id": "5628d0ae-6d2a-4976-8947-9d8c3d1f5ce3"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94702df8-3bed-45b1-88e8-7a1370e084ed",
      "metadata": {
        "id": "94702df8-3bed-45b1-88e8-7a1370e084ed"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Input(s):\n",
        "        batch: list of tuples (audio_tensors, label)\n",
        "            - audio_tensors: [num_recordings, T], where T is not fixed for r in R.\n",
        "    \"\"\"\n",
        "    audios_list, labels_list = zip(*batch)\n",
        "    B = len(audios_list)\n",
        "    R = len(audios_list[0])\n",
        "\n",
        "    # ----- max len per batch -----\n",
        "    max_len = max(aud.shape[-1] for subject in audios_list for aud in subject)\n",
        "\n",
        "    padded_audios = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for subject in audios_list: # subject : [R, T]\n",
        "        subject_padded = []\n",
        "        subject_mask = []\n",
        "\n",
        "        for aud in subject: # aud [T] or [1, T]\n",
        "            if aud.ndim == 2:\n",
        "                aud = aud.squeeze(0) # remove channel_dim if [1, T]\n",
        "            L = aud.shape[0]\n",
        "            pad_amount = max_len - L\n",
        "            if pad_amount > 0:\n",
        "                aud = torch.nn.functional.pad(aud, (0, pad_amount))\n",
        "                mask = torch.cat([torch.ones(L), torch.zeros(pad_amount)])\n",
        "            else:\n",
        "                mask = torch.ones(L)\n",
        "\n",
        "            subject_padded.append(aud) # [T_max]\n",
        "            subject_mask.append(mask) # [T_max]\n",
        "\n",
        "        padded_audios.append(torch.stack(subject_padded)) # [R, T_max]\n",
        "        attention_masks.append(torch.stack(subject_mask)) # [R, T_max]\n",
        "\n",
        "    # [B, R, T]\n",
        "    padded_audios = torch.stack(padded_audios)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    labels = torch.tensor(labels_list)\n",
        "\n",
        "    return padded_audios, attention_masks, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GTDFRE4NHQBb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTDFRE4NHQBb",
        "outputId": "0f19c3b6-57a0-4237-cc5e-b1a592913851"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_labels = []\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "    _, label = train_dataset[i]\n",
        "    train_labels.append(int(label))\n",
        "\n",
        "# count classes\n",
        "class_sample_counts = np.bincount(train_labels)\n",
        "print(\"train counts:\", class_sample_counts)\n",
        "\n",
        "\n",
        "train_freq = class_sample_counts / class_sample_counts.sum()\n",
        "\n",
        "# inverse frequency weights\n",
        "weights = 1.0 / class_sample_counts\n",
        "weights /= weights.mean()\n",
        "\n",
        "sample_weights = weights[train_labels]\n",
        "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282bddd9-60c2-4781-9744-adb8f62c2910",
      "metadata": {
        "id": "282bddd9-60c2-4781-9744-adb8f62c2910"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], num_workers=4, pin_memory=True, collate_fn=collate_fn, sampler=sampler)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97737395-2082-4d36-b952-08afae53d6bf",
      "metadata": {
        "id": "97737395-2082-4d36-b952-08afae53d6bf"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892e10b5-00e9-4497-9fce-dcd31d6cfaa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "892e10b5-00e9-4497-9fce-dcd31d6cfaa7",
        "outputId": "257ac15e-eef6-4a9a-cc3f-65f84b10cabb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, data in enumerate(val_loader):\n",
        "    wavs, attn_mask, labels = data\n",
        "\n",
        "    print(\"wavs shape:\", wavs.shape)          # [B, R, T]\n",
        "    print(\"attn_mask shape:\", attn_mask.shape)\n",
        "    print(\"labels shape:\", labels.shape)\n",
        "    print(\"labels:\", labels)\n",
        "\n",
        "    # Visualize the first audio example\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(wavs[0][0].numpy())\n",
        "    plt.title(\"Raw Audio Waveform\")\n",
        "    plt.xlabel(\"Time (samples)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.show()\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e845d2e-87be-44f5-bcae-5f9e3da343f3",
      "metadata": {
        "id": "5e845d2e-87be-44f5-bcae-5f9e3da343f3"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32c0781-c751-430c-a469-81c91b74f966",
      "metadata": {
        "id": "a32c0781-c751-430c-a469-81c91b74f966"
      },
      "source": [
        "### Wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d97a99e-545e-4879-9514-69a917b87bb9",
      "metadata": {
        "id": "8d97a99e-545e-4879-9514-69a917b87bb9"
      },
      "outputs": [],
      "source": [
        "class Wav2VecVanilla(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(WAV2VEC_MODEL,\n",
        "                                                   output_hidden_states=False,\n",
        "                                                   cache_dir=CACHE_DIR)\n",
        "        self.output_dim = self.wav2vec.config.hidden_size\n",
        "\n",
        "    def _compute_feat_mask(self, attention_mask):\n",
        "        \"\"\"\n",
        "        Convert input attention mask [B, T_samples] into\n",
        "        downsampled feature mask [B, T_feat].\n",
        "        \"\"\"\n",
        "        device = attention_mask.device\n",
        "        B, T = attention_mask.shape\n",
        "\n",
        "        input_lengths = attention_mask.sum(dim=1)  # [B]\n",
        "\n",
        "        feat_lengths = self.wav2vec._get_feat_extract_output_lengths(input_lengths)\n",
        "\n",
        "        max_feat_len = self.wav2vec._get_feat_extract_output_lengths(\n",
        "            torch.tensor([T], device=device)\n",
        "        )[0]\n",
        "\n",
        "        feat_mask = torch.arange(max_feat_len, device=device)[None, :] < feat_lengths[:, None]\n",
        "        return feat_mask  # bool or uint8, shape [B, T_feat]\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        out = self.wav2vec(wavs, attention_mask=attention_mask)\n",
        "        hidden = out.last_hidden_state # [B, T_feat, 768]\n",
        "        feat_mask = self._compute_feat_mask(attention_mask)\n",
        "        return hidden, feat_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93317f35-544d-48c1-a595-9bfb92936db4",
      "metadata": {
        "id": "93317f35-544d-48c1-a595-9bfb92936db4"
      },
      "outputs": [],
      "source": [
        "class Wav2VecLoRA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base = Wav2Vec2Model.from_pretrained(WAV2VEC_MODEL,\n",
        "                                           output_hidden_states=False,\n",
        "                                           cache_dir=CACHE_DIR)\n",
        "        self.output_dim = self.base.config.hidden_size\n",
        "\n",
        "        # ----- LoRA -----\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=config.get(\"lora_r\", 1),\n",
        "            lora_alpha=config.get(\"lora_alpha\", 16),\n",
        "            target_modules=config.get(\"target_modules\", [\"q_proj\", \"k_proj\", \"v_proj\"]), # can also do just [\"q_proj\"]\n",
        "            lora_dropout=0.0,\n",
        "            bias=\"none\"\n",
        "        )\n",
        "\n",
        "        self.wav2vec = get_peft_model(self.base, lora_cfg)\n",
        "        self.output_dime = self.base.config.hidden_size\n",
        "\n",
        "\n",
        "    def _compute_feat_mask(self, attention_mask):\n",
        "        \"\"\"\n",
        "        Convert input attention mask [B, T_samples] into\n",
        "        downsampled feature mask [B, T_feat].\n",
        "        \"\"\"\n",
        "        B, T = attention_mask.shape\n",
        "        device = attention_mask.device\n",
        "\n",
        "        input_lengths = attention_mask.sum(dim=1)  # [B]\n",
        "\n",
        "        feat_lengths = self.base._get_feat_extract_output_lengths(input_lengths)\n",
        "\n",
        "        # Max possible T_feat for this batch\n",
        "        max_feat_len = self.base._get_feat_extract_output_lengths(\n",
        "            torch.tensor([T], device=device)\n",
        "        )[0]\n",
        "\n",
        "        feat_mask = torch.arange(max_feat_len, device=device)[None, :] < feat_lengths[:, None]\n",
        "        return feat_mask  # [B, T_feat]\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        out = self.wav2vec(wavs, attention_mask=attention_mask)\n",
        "        hidden = out.last_hidden_state # [B, T_feat, 768]\n",
        "        feat_mask = self._compute_feat_mask(attention_mask)  # [B, T_feat]\n",
        "        return hidden, feat_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d52c668-d28d-4d94-9640-e3393e4c1c61",
      "metadata": {
        "id": "6d52c668-d28d-4d94-9640-e3393e4c1c61"
      },
      "outputs": [],
      "source": [
        "class Wav2VecWrapper(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        fe = config[\"feature_extractor\"]\n",
        "        if fe == \"wav2vec_vanilla\":\n",
        "            self.feature_extractor = Wav2VecVanilla(config)\n",
        "        elif fe == \"wav2vec_lora\":\n",
        "            self.feature_extractor = Wav2VecLoRA(config)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown feature extractor: {fe}\")\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "        B, R, T = wavs.shape\n",
        "        # assert R == self.R, f\"Expected R={self.R}, got {R}\"\n",
        "\n",
        "        # ----- Flatten batch*recordings for parallel Wav2Vec -----\n",
        "        wavs_flat = wavs.reshape(B*R, T)\n",
        "        mask_flat = attention_mask.reshape(B*R, T)\n",
        "\n",
        "        out, padding_mask = self.feature_extractor(wavs_flat, mask_flat)  # [B*R, T_feat, D], [B*R, T_feat]\n",
        "        out = out.reshape(B, R, -1, self.feature_extractor.output_dim) # Reshape to [B, R, T_feat]\n",
        "        padding_mask = padding_mask.reshape(B, R, -1) # Reshape to [B, R, T_feat]\n",
        "        assert out.shape[2] == padding_mask.shape[2], f\"Expected second dimension of out={out.shape} == padding_mask={padding_mask.shape}\"\n",
        "        return out, padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c178bd85-62f8-4ed4-9bf5-66eba42a7604",
      "metadata": {
        "id": "c178bd85-62f8-4ed4-9bf5-66eba42a7604"
      },
      "source": [
        "### Feature Extractor from Wav2Vec Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895868e6-beea-46b5-9148-e1a9b3acbfed",
      "metadata": {
        "id": "895868e6-beea-46b5-9148-e1a9b3acbfed"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SeqFeatureExtractorWrapper(nn.Module):\n",
        "    def __init__(self, config, num_heads=4):\n",
        "        \"\"\"\n",
        "        Pools only across the time dimension T.\n",
        "        Input:  (B, T, D)\n",
        "        Output: (B, D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mode = config[\"post_processor\"]\n",
        "        if self.mode is None or self.mode == \"none\":\n",
        "          self.mode = \"last\"\n",
        "\n",
        "        elif self.mode == \"attention\":\n",
        "            self.att = nn.Linear(config[\"attention_dim\"], 1)\n",
        "\n",
        "        elif self.mode == \"conv\":\n",
        "            self.conv = nn.Conv1d(config[\"feature_dim\"], config[\"feature_dim\"], kernel_size=config[\"conv_kernel_size\"], padding=config[\"conv_padding\"])\n",
        "\n",
        "        elif self.mode == \"multihead\":\n",
        "            self.query = nn.Parameter(torch.randn(1, 1, config[\"attention_dim\"]))\n",
        "            self.att_mh = nn.MultiheadAttention(config[\"attention_dim\"], num_heads, batch_first=True)\n",
        "\n",
        "    def post_processor(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        mask: (B, T)   True = valid, False = pad\n",
        "        \"\"\"\n",
        "        # --------------------\n",
        "        # LAST-HIDDEN\n",
        "        # --------------------\n",
        "        if self.mode == \"last\":\n",
        "            idx = mask.float().flip(1).argmax(1)\n",
        "            idx[idx == 0] = mask.size(1) - 1\n",
        "            return x[torch.arange(x.size(0)), idx]\n",
        "\n",
        "        # --------------------\n",
        "        # MEAN\n",
        "        # --------------------\n",
        "        if self.mode == \"mean\":\n",
        "            x_masked = x * mask.unsqueeze(-1)\n",
        "            lengths = mask.sum(1).clamp(min=1).unsqueeze(-1)\n",
        "            return (x_masked.sum(1) / lengths)\n",
        "\n",
        "        # --------------------\n",
        "        # MAX\n",
        "        # --------------------\n",
        "        if self.mode == \"max\":\n",
        "            x_masked = x.masked_fill(~mask.unsqueeze(-1), -1e9)\n",
        "            return x_masked.max(1)[0]\n",
        "\n",
        "        # --------------------\n",
        "        # ATTENTION\n",
        "        # --------------------\n",
        "        if self.mode == \"attention\":\n",
        "            scores = self.att(x).squeeze(-1)\n",
        "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
        "            weights = torch.softmax(scores, 1).unsqueeze(-1)\n",
        "            return (weights * x).sum(1)\n",
        "\n",
        "        # --------------------\n",
        "        # CONV\n",
        "        # --------------------\n",
        "        if self.mode == \"conv\":\n",
        "            z = x.transpose(1, 2)          # (B, D, T)\n",
        "            z = torch.relu(self.conv(z))   # (B, D, T)\n",
        "            return z.mean(-1)              # (B, D)\n",
        "\n",
        "        # --------------------\n",
        "        # MULTIHEAD ATTENTION\n",
        "        # --------------------\n",
        "        if self.mode == \"multihead\":\n",
        "            B = x.size(0)\n",
        "            q = self.query.repeat(B, 1, 1)\n",
        "            pooled, _ = self.att_mh(q, x, x, key_padding_mask=~mask)\n",
        "            return pooled.squeeze(1)\n",
        "\n",
        "        raise ValueError(self.mode)\n",
        "    def forward(self, x, padding_mask):\n",
        "      B, R, T, D = x.shape\n",
        "      assert x.shape[:3] == padding_mask.shape, f\"Expected x.shape[0:3]={x.shape}==padding_mask.shape={padding_mask.shape}\"\n",
        "      x_flat = x.reshape(B*R, T, D)\n",
        "      mask_flat = padding_mask.reshape(B*R, T)\n",
        "      # ----- Eliminate Time Axis -----\n",
        "      out = self.post_processor(x_flat, mask_flat) # [B*R, D]\n",
        "      out = out.reshape(B, R, -1) # [B, R, D]\n",
        "\n",
        "      # ----- Fuse Multiple Recordings -----\n",
        "      out = out.mean(dim=1) # [B, D]\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92a4065-dfe5-4fc4-aa86-3b1687701199",
      "metadata": {
        "id": "e92a4065-dfe5-4fc4-aa86-3b1687701199"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc3e140-fa80-490d-b870-52bf3fc835c8",
      "metadata": {
        "id": "dfc3e140-fa80-490d-b870-52bf3fc835c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        D = config.get(\"hidden_dim\", 768)\n",
        "        num_classes = config[\"num_classes\"]\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(D, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5346efd2-e66a-46eb-9174-2c047cd0f513",
      "metadata": {
        "id": "5346efd2-e66a-46eb-9174-2c047cd0f513"
      },
      "source": [
        "### ALS Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fafecb1-8d84-42a6-a1ec-a94da1aaeddf",
      "metadata": {
        "id": "2fafecb1-8d84-42a6-a1ec-a94da1aaeddf"
      },
      "outputs": [],
      "source": [
        "class ALSDetectionModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings_extractor = Wav2VecWrapper(config)\n",
        "        self.sequence_postprocessor = SeqFeatureExtractorWrapper(config)\n",
        "        self.classifier = MLPClassifier(config)\n",
        "\n",
        "    def forward(self, wavs, attention_mask):\n",
        "\n",
        "        out, padding_mask = self.embeddings_extractor(wavs, attention_mask)\n",
        "\n",
        "        out = self.sequence_postprocessor(out, padding_mask)\n",
        "\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff9324f-5bb0-4b5a-ad42-bdb9eff40c7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9f4ee6d5db9846cabcd628ce95ce9bbf",
            "9edf552a88ce4c3fa50073fa7dfd997b",
            "b612c3f966024a30b316bf067028ed2f",
            "c768dedc3f0b41258d296317e4f75084",
            "2ea80b3dba0a4d7e94e640726a51d3fb",
            "954a154c38e440ef95b871423b22b562",
            "50d2bf05e07f4e18a9f952880223ef49",
            "e2ebf17986124c07bde29f2d07e550af",
            "5a150cb14420444aaaab8b6e706fb8d9",
            "6e888ea29158433ea3e7c39b814ea8aa",
            "ade63c15c5f7417697039d86099819f2"
          ]
        },
        "id": "4ff9324f-5bb0-4b5a-ad42-bdb9eff40c7a",
        "outputId": "4856ad8e-f606-4dd9-b1c6-4b87b68bb231"
      },
      "outputs": [],
      "source": [
        "model = ALSDetectionModel(config).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd07c30-0413-4e93-a6b6-b27b6cdee5b0",
      "metadata": {
        "id": "1bd07c30-0413-4e93-a6b6-b27b6cdee5b0"
      },
      "outputs": [],
      "source": [
        "B, P, T = 2, 8, 16000\n",
        "x = torch.randn(B, P, T).to(DEVICE)\n",
        "mask = torch.ones_like(x, dtype=torch.long)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(x, mask)\n",
        "\n",
        "print(\"Logits shape:\", out.shape)  # Expected: [B, num_classes] â†’ [2, num_classes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s_hucDXZXUnX",
      "metadata": {
        "id": "s_hucDXZXUnX"
      },
      "outputs": [],
      "source": [
        "# get class counts\n",
        "'''\n",
        "num_classes = 5\n",
        "counts = torch.zeros(num_classes)\n",
        "\n",
        "for i , (_, _, labels) in tqdm(enumerate(train_loader), total= len(train_loader)):\n",
        "    for label in labels:\n",
        "        counts[label.item()] += 1\n",
        "\n",
        "total = counts.sum()\n",
        "freq = counts / total\n",
        "print(\"train class_freq:\", freq.tolist())\n",
        "'''\n",
        "train_freq = [0.021052632480859756, 0.10000000149011612, 0.20000000298023224, 0.28421053290367126, 0.3947368562221527]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d877dce5-c58e-4fe6-b7c8-c9dd71d6801a",
      "metadata": {
        "id": "d877dce5-c58e-4fe6-b7c8-c9dd71d6801a"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Loss function:\n",
        "\n",
        "# Weighted CE:\n",
        "# create class weights (inverse sqrt of frequency)\n",
        "class_freq = torch.tensor(train_freq, dtype=torch.float32)\n",
        "class_weights = 1.0 / torch.sqrt(class_freq)\n",
        "class_weights_norm = class_weights / class_weights.mean()\n",
        "print(class_weights_norm)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_norm.to(DEVICE), label_smoothing=0.05)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr = 1e-6)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63278aa-3947-43d7-b45c-afe3a925ed64",
      "metadata": {
        "id": "a63278aa-3947-43d7-b45c-afe3a925ed64"
      },
      "source": [
        "### Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b974c1f-66e9-4970-b1fc-185c2816aefc",
      "metadata": {
        "id": "4b974c1f-66e9-4970-b1fc-185c2816aefc"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, scaler):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0.0, 0.0\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (audio_tensors, attention_mask, labels) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        audio_tensors = audio_tensors.to(DEVICE)      # [B, R, T]\n",
        "        attention_mask = attention_mask.to(DEVICE)    # [B, R, T]\n",
        "        labels = labels.to(DEVICE)                    # [B]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
        "            logits = model(audio_tensors, attention_mask)  # [B, num_classes]\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tloss += loss.item()\n",
        "        tacc += (torch.argmax(logits, dim=1) == labels).sum().item() / labels.size(0)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.4f}\".format(tloss / (i + 1)),\n",
        "            acc=\"{:.4f}%\".format(tacc * 100 / (i + 1))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del audio_tensors, attention_mask, labels, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss /= len(dataloader)\n",
        "    tacc /= len(dataloader)\n",
        "\n",
        "    return tloss, tacc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c5955f-bb40-40f5-86b6-38d0f80bbe7b",
      "metadata": {
        "id": "02c5955f-bb40-40f5-86b6-38d0f80bbe7b"
      },
      "outputs": [],
      "source": [
        "f1 = F1Score(num_classes=config[\"num_classes\"], average='macro', task='multiclass').cpu()\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    vloss, vacc = 0.0, 0.0\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (audio_tensors, attention_mask, labels) in enumerate(dataloader):\n",
        "        audio_tensors = audio_tensors.to(DEVICE)     # [B, R, T]\n",
        "        attention_mask = attention_mask.to(DEVICE)   # [B, R, T]\n",
        "        labels = labels.to(DEVICE)                   # [B]\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            logits = model(audio_tensors, attention_mask)  # [B, num_classes]\n",
        "            loss = criterion(logits, labels)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        vloss += loss.item()\n",
        "        vacc += (preds == labels).sum().item() / labels.size(0)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.4f}\".format(vloss / (i + 1)),\n",
        "            acc=\"{:.4f}%\".format(vacc * 100 / (i + 1))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del audio_tensors, attention_mask, labels, logits, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    vloss /= len(dataloader)\n",
        "    vacc /= len(dataloader)\n",
        "\n",
        "    # concatenate all predictions and labels\n",
        "    all_preds = torch.cat(all_preds).cpu()\n",
        "    all_labels = torch.cat(all_labels).cpu()\n",
        "\n",
        "    vf1 = f1(all_preds.cpu(), all_labels.cpu()).cpu()  # make sure f1 accepts CPU tensors\n",
        "\n",
        "    # log classification report as well\n",
        "    print(classification_report(all_labels, all_preds, digits=3))\n",
        "\n",
        "    del all_preds, all_labels\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return vloss, vacc, vf1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada5b91b-b018-4e96-85cb-51fa4e1d7354",
      "metadata": {
        "id": "ada5b91b-b018-4e96-85cb-51fa4e1d7354"
      },
      "source": [
        "### Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3d24fc-6a2e-4534-ad48-4cefa3feb276",
      "metadata": {
        "id": "5f3d24fc-6a2e-4534-ad48-4cefa3feb276"
      },
      "outputs": [],
      "source": [
        "# add api key\n",
        "wandb.login(key=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4409c277-27f9-4e45-8c33-262c1ea07461",
      "metadata": {
        "id": "4409c277-27f9-4e45-8c33-262c1ea07461"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "RESUME_OLD_RUN = False\n",
        "\n",
        "if RESUME_OLD_RUN == True:\n",
        "    print(\"Resuming previous WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        #id     = None,\n",
        "        resume = \"must\",\n",
        "        project = \"SAND\",\n",
        "        config  = config\n",
        "    )\n",
        "else:\n",
        "    print(\"Initializing new WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        reinit  = True,\n",
        "        project = 'SAND',\n",
        "        config  = config\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LjYdqtvC-t-Z",
      "metadata": {
        "id": "LjYdqtvC-t-Z"
      },
      "outputs": [],
      "source": [
        "p = model.embeddings_extractor.feature_extractor.base.masked_spec_embed\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Pick one of these:\n",
        "\n",
        "    # Option 1: zero init (totally fine for this embedding)\n",
        "    torch.nn.init.zeros_(p)\n",
        "\n",
        "    # Option 2: normal init\n",
        "    # torch.nn.init.normal_(p, mean=0.0, std=0.02)\n",
        "\n",
        "print(\"masked_spec_embed after manual init:\",\n",
        "      \"nan\", torch.isnan(p).any().item(),\n",
        "      \"inf\", torch.isinf(p).any().item(),\n",
        "      \"min\", p.min().item(),\n",
        "      \"max\", p.max().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72zAbrvA7cZ5",
      "metadata": {
        "id": "72zAbrvA7cZ5"
      },
      "outputs": [],
      "source": [
        "p = model.embeddings_extractor.feature_extractor.base.masked_spec_embed\n",
        "print(\"masked_spec_embed pre-train:\",\n",
        "      \"nan\", torch.isnan(p).any().item(),\n",
        "      \"inf\", torch.isinf(p).any().item(),\n",
        "      \"min\", p.min().item(),\n",
        "      \"max\", p.max().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb967bd8-9a8b-49c0-88ca-c356a7848107",
      "metadata": {
        "id": "bb967bd8-9a8b-49c0-88ca-c356a7848107"
      },
      "outputs": [],
      "source": [
        "best_val_f1 = 0.0\n",
        "run_name = f'ablation: {config['ablation_ID']}'\n",
        "os.makedirs(run_name, exist_ok=True)\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, scaler)\n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"\\tTrain Acc {train_acc*100:.04f}%\\tTrain Loss {train_loss:.04f}\\tLR {curr_lr:.7f}\")\n",
        "    print(f\"\\tVal Acc {val_acc*100:.04f}%\\tVal Loss {val_loss:.04f}\\tF1 {val_f1:.04f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "        'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr, 'valid_F1': val_f1\n",
        "    })\n",
        "\n",
        "    if epoch >= 10:\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_f1': best_val_f1,\n",
        "            'train_acc': train_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch+1}.pth')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Saved new best model with Val F1: {val_f1} at {checkpoint_path}\")\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Q0-9g4QySub",
      "metadata": {
        "id": "6Q0-9g4QySub"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for audio_tensors, attention_mask, labels in val_loader:\n",
        "        audio_tensors = audio_tensors.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        logits = model(audio_tensors, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "# concatenate across batches\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_labels = torch.cat(all_labels).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5A-BR7XvyuTN",
      "metadata": {
        "id": "5A-BR7XvyuTN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "print(classification_report(all_labels, all_preds, digits=3))\n",
        "print(\"Macro F1:\", f1_score(all_labels, all_preds, average='macro'))\n",
        "print(\"Weighted F1:\", f1_score(all_labels, all_preds, average='weighted'))\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ea80b3dba0a4d7e94e640726a51d3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d2bf05e07f4e18a9f952880223ef49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a150cb14420444aaaab8b6e706fb8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e888ea29158433ea3e7c39b814ea8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "954a154c38e440ef95b871423b22b562": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9edf552a88ce4c3fa50073fa7dfd997b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954a154c38e440ef95b871423b22b562",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_50d2bf05e07f4e18a9f952880223ef49",
            "value": "model.safetensors:â€‡â€‡51%"
          }
        },
        "9f4ee6d5db9846cabcd628ce95ce9bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9edf552a88ce4c3fa50073fa7dfd997b",
              "IPY_MODEL_b612c3f966024a30b316bf067028ed2f",
              "IPY_MODEL_c768dedc3f0b41258d296317e4f75084"
            ],
            "layout": "IPY_MODEL_2ea80b3dba0a4d7e94e640726a51d3fb"
          }
        },
        "ade63c15c5f7417697039d86099819f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b612c3f966024a30b316bf067028ed2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2ebf17986124c07bde29f2d07e550af",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a150cb14420444aaaab8b6e706fb8d9",
            "value": 192587935
          }
        },
        "c768dedc3f0b41258d296317e4f75084": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e888ea29158433ea3e7c39b814ea8aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ade63c15c5f7417697039d86099819f2",
            "value": "â€‡193M/378Mâ€‡[01:00&lt;00:22,â€‡8.35MB/s]"
          }
        },
        "e2ebf17986124c07bde29f2d07e550af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
