{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f8ee69c-bdb8-4078-bbfc-7f5810b3faf3",
      "metadata": {
        "id": "3f8ee69c-bdb8-4078-bbfc-7f5810b3faf3"
      },
      "source": [
        "# Mid-Term Report Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fa1205-420e-4551-b931-dd46423ced5a",
      "metadata": {
        "id": "d0fa1205-420e-4551-b931-dd46423ced5a"
      },
      "source": [
        "This notebook comprises the required experiments using the SAND dataset/challenge. The notebook consists of the following sections: 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d08aa8-0474-4ce5-8e9e-915b4c2a9686",
      "metadata": {
        "id": "54d08aa8-0474-4ce5-8e9e-915b4c2a9686"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d6d8df-f383-40e0-9b61-51dd37bc289b",
      "metadata": {
        "id": "f1d6d8df-f383-40e0-9b61-51dd37bc289b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f85ca3-9cc3-4cb9-a506-ecf3d758e7ad",
      "metadata": {
        "id": "16f85ca3-9cc3-4cb9-a506-ecf3d758e7ad"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet # Install WandB\n",
        "!pip install pytorch_metric_learning --quiet # Install the Pytorch Metric Library\n",
        "!pip install torchinfo --quiet # Install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100b3f9e-593b-4224-9e07-85d4ca3c720c",
      "metadata": {
        "id": "100b3f9e-593b-4224-9e07-85d4ca3c720c"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1155d00-7cc9-43f4-8cf7-81af4330c507",
      "metadata": {
        "id": "c1155d00-7cc9-43f4-8cf7-81af4330c507"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchaudio\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from pathlib import Path\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import samplers\n",
        "import csv\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4d7aed-ce58-457b-b24e-1f244b798471",
      "metadata": {
        "id": "bf4d7aed-ce58-457b-b24e-1f244b798471"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db863a4d-8286-43c3-be99-b37700bb6b2f",
      "metadata": {
        "id": "db863a4d-8286-43c3-be99-b37700bb6b2f"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    ## Problem Configs\n",
        "    \"subset\" : 1,\n",
        "    \"task_num\" : 1,\n",
        "    \"fs\" : 8000, # in Hz\n",
        "    \"max_len\" : 5, # in seconds\n",
        "    ## Data Configs\n",
        "    \"train_dir\" : \"/content/drive/MyDrive/Fall2025/11685/SAND_Challenge/Dataset/training_split_balanced/train\",\n",
        "    \"val_dir\" :  \"/content/drive/MyDrive/Fall2025/11685/SAND_Challenge/Dataset/training_split_balanced/val\",\n",
        "    \"test_dir\" :  \"/content/drive/MyDrive/Fall2025/11685/SAND_Challenge/Dataset/test\",\n",
        "    ## Model Configs\n",
        "    \"model\" : \"ViT_baseline\",\n",
        "    ## Training Configs\n",
        "    'batch_size': 16, # Increase this if your GPU can handle it\n",
        "    'lr': 1e-6,\n",
        "    \"weight_decay\" : 1e-4,\n",
        "    'epochs': 50,\n",
        "    'num_classes': 5,\n",
        "    'checkpoint_dir': \"/content/drive/MyDrive/Fall2025/11685/SAND_Challenge/checkpoints\",\n",
        "    'augument': True,\n",
        "    'ablation_ID': 22\n",
        "}\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FcuBLMyUqDDT"
      },
      "id": "FcuBLMyUqDDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6c1074-68f6-4b4d-ba21-d6541dce2a1a",
      "metadata": {
        "id": "df6c1074-68f6-4b4d-ba21-d6541dce2a1a"
      },
      "outputs": [],
      "source": [
        "phonation_list = [\"phonationA\",\n",
        "                  \"phonationE\",\n",
        "                  \"phonationI\",\n",
        "                  \"phonationO\",\n",
        "                  \"phonationU\",\n",
        "                  \"rhythmKA\",\n",
        "                  \"rhythmPA\",\n",
        "                  \"rhythmTA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1087aa-238c-49c3-94f4-80b98eff4e33",
      "metadata": {
        "id": "9b1087aa-238c-49c3-94f4-80b98eff4e33"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31fd3cf1-6cfa-4c09-b800-934a2d5a8b24",
      "metadata": {
        "id": "31fd3cf1-6cfa-4c09-b800-934a2d5a8b24"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16830ab6-6ffe-4418-b5cd-4d0058069b18",
      "metadata": {
        "id": "16830ab6-6ffe-4418-b5cd-4d0058069b18"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103a656b-5486-41fb-9e56-43ce62f55c1e",
      "metadata": {
        "id": "103a656b-5486-41fb-9e56-43ce62f55c1e"
      },
      "source": [
        "Change this to appropriate speech augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecda6af-3bf5-42fa-9e9b-948d12498944",
      "metadata": {
        "id": "9ecda6af-3bf5-42fa-9e9b-948d12498944"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as AT\n",
        "\n",
        "def create_audio_transforms(sample_rate: int = 8000, n_mels: int = 64, augment: bool = True) -> torch.nn.Sequential:\n",
        "    \"\"\"\n",
        "    Create transform pipeline for audio classification tasks.\n",
        "\n",
        "    Args:\n",
        "        sample_rate (int): Sample rate of the audio signals.\n",
        "        n_mels (int): Number of Mel filterbanks.\n",
        "        augment (bool): Whether to apply data augmentation.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Sequential: Audio transform pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    transform_list = []\n",
        "\n",
        "    # Step 1: Convert waveform to Mel Spectrogram\n",
        "    transform_list.append(\n",
        "        AT.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=1024,\n",
        "            hop_length=512,\n",
        "            n_mels=n_mels\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Step 2: Apply log transformation to compress dynamic range\n",
        "    transform_list.append(AT.AmplitudeToDB())  # Log-scale the Mel spectrogram\n",
        "\n",
        "    # Step 3: (Optional) Data augmentation for spectrograms\n",
        "    if augment:\n",
        "        transform_list.extend([\n",
        "            AT.FrequencyMasking(freq_mask_param=8),\n",
        "            AT.TimeMasking(time_mask_param=20),\n",
        "        ])\n",
        "\n",
        "    # Return the composed transformation pipeline\n",
        "    return torch.nn.Sequential(*transform_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1352e715-9d1d-4a6e-864a-5e670b08898f",
      "metadata": {
        "id": "1352e715-9d1d-4a6e-864a-5e670b08898f"
      },
      "source": [
        "### Task 1 Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022363a0-7803-4f5b-aea2-5fb86b85bd5a",
      "metadata": {
        "id": "022363a0-7803-4f5b-aea2-5fb86b85bd5a"
      },
      "outputs": [],
      "source": [
        "class AudioMultiPhonationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for loading multiple phonation audio samples per subject with class labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, phonation_types, transform=None, metadata_file=\"sand_task_1.xlsx\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the root directory containing phonation folders.\n",
        "            phonation_types (list): List of phonation or rhythm folder names (e.g., [\"PhonationA\", \"PhonationE\", ...]).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample (e.g., waveform -> spectrogram).\n",
        "            metadata_file (str): Path to Excel file containing ID -> class mappings.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.phonation_types = phonation_types\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load metadata from Excel file\n",
        "        metadata_path = os.path.join(root_dir, metadata_file)\n",
        "        self.metadata_df = pd.read_excel(metadata_path)\n",
        "\n",
        "        # Map ID to label (class)\n",
        "        self.id_to_label = dict(zip(self.metadata_df['ID'], self.metadata_df['Class']))\n",
        "\n",
        "        # Build a list of available IDs that have audio in all phonation folders\n",
        "        self.subject_ids = self._collect_valid_subject_ids()\n",
        "\n",
        "\n",
        "    def _collect_valid_subject_ids(self):\n",
        "        \"\"\"\n",
        "        Collect subject IDs that have a complete set of phonation files.\n",
        "        \"\"\"\n",
        "        phonation_files = {}\n",
        "        for phonation in self.phonation_types:\n",
        "            phonation_path = os.path.join(self.root_dir, phonation)\n",
        "            files = os.listdir(phonation_path)\n",
        "            ids = set(f.split('_')[0] for f in files if f.endswith('.wav'))\n",
        "            phonation_files[phonation] = ids\n",
        "\n",
        "        # Keep only IDs that are present in all phonation folders\n",
        "        valid_ids = set.intersection(*phonation_files.values())\n",
        "        # Also make sure the ID exists in the metadata\n",
        "        valid_ids = valid_ids.intersection(set(str(id_) for id_ in self.metadata_df['ID']))\n",
        "        return sorted(valid_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        subject_id = self.subject_ids[idx]\n",
        "        audio_tensors = []\n",
        "        target_length = config[\"max_len\"] * config[\"fs\"]\n",
        "\n",
        "        for phonation in self.phonation_types:\n",
        "            filename = f\"{subject_id}_{phonation}.wav\"\n",
        "            filepath = os.path.join(self.root_dir, phonation, filename)\n",
        "            waveform, sample_rate = torchaudio.load(filepath)\n",
        "\n",
        "            num_samples = waveform.shape[1]\n",
        "            if num_samples > target_length:\n",
        "                waveform = waveform[:, :target_length]\n",
        "            elif num_samples < target_length:\n",
        "                pad_amount = target_length - num_samples\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "            if self.transform:\n",
        "                waveform = self.transform(waveform)\n",
        "\n",
        "            audio_tensors.append(waveform)\n",
        "\n",
        "        # final tensor shape: (P, 1, F, T)\n",
        "        audio_tensors = torch.stack(audio_tensors).to(dtype=torch.float32)\n",
        "\n",
        "        # scalar label (IMPORTANT: python int first)\n",
        "        label = int(self.id_to_label[subject_id] - 1)\n",
        "\n",
        "        return audio_tensors, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f645a50-98a9-42a9-a395-9fe0b2489852",
      "metadata": {
        "id": "1f645a50-98a9-42a9-a395-9fe0b2489852"
      },
      "outputs": [],
      "source": [
        "class AudioMultiPhonationDataset_Test(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for loading multiple phonation audio samples per subject with class labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, phonation_types, transform=None, metadata_file=\"sand_task1_test.xlsx\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the root directory containing phonation folders.\n",
        "            phonation_types (list): List of phonation or rhythm folder names (e.g., [\"PhonationA\", \"PhonationE\", ...]).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample (e.g., waveform -> spectrogram).\n",
        "            metadata_file (str): Path to Excel file containing ID -> class mappings.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.phonation_types = phonation_types\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load metadata from Excel file\n",
        "        metadata_path = os.path.join(root_dir, metadata_file)\n",
        "        self.metadata_df = pd.read_excel(metadata_path)\n",
        "\n",
        "        # Map ID to label (class)\n",
        "        # We don't have classes yet\n",
        "        # self.id_to_label = dict(zip(self.metadata_df['ID'], self.metadata_df['Class']))\n",
        "\n",
        "\n",
        "        # Build a list of available IDs that have audio in all phonation folders\n",
        "\n",
        "        self.subject_ids = self._collect_valid_subject_ids()\n",
        "\n",
        "\n",
        "    def _collect_valid_subject_ids(self):\n",
        "        \"\"\"\n",
        "        Collect subject IDs that have a complete set of phonation files.\n",
        "        \"\"\"\n",
        "        phonation_files = {}\n",
        "        for phonation in self.phonation_types:\n",
        "            phonation_path = os.path.join(self.root_dir, phonation)\n",
        "            files = os.listdir(phonation_path)\n",
        "            ids = set(f.split('_')[0] for f in files if f.endswith('.wav'))\n",
        "            phonation_files[phonation] = ids\n",
        "\n",
        "        # Keep only IDs that are present in all phonation folders\n",
        "        valid_ids = set.intersection(*phonation_files.values())\n",
        "\n",
        "        # Filter by metadata ID column ONLY (no class filtering)\n",
        "        # pad with ID in front to prevent ID format issues\n",
        "        metadata_ids = set(\"ID\" + str(id_).zfill(3) for id_ in self.metadata_df['ID'])\n",
        "\n",
        "        valid_ids = valid_ids.intersection(metadata_ids)\n",
        "\n",
        "        return sorted(valid_ids)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            tuple: (list of waveforms or transformed tensors, class label)\n",
        "        \"\"\"\n",
        "        subject_id = self.subject_ids[idx]\n",
        "        audio_tensors = []\n",
        "        target_length = config[\"max_len\"] * config[\"fs\"]\n",
        "\n",
        "        for phonation in self.phonation_types:\n",
        "            filename = f\"{subject_id}_{phonation}.wav\"\n",
        "            filepath = os.path.join(self.root_dir, phonation, filename)\n",
        "\n",
        "            waveform, sample_rate = torchaudio.load(filepath)\n",
        "\n",
        "            num_samples = waveform.shape[1]\n",
        "            if num_samples > target_length:\n",
        "                waveform = waveform[:, :target_length]  # Truncate\n",
        "            elif num_samples < target_length:\n",
        "                pad_amount = target_length - num_samples\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, pad_amount))  # Pad at the end\n",
        "\n",
        "\n",
        "            # Optionally apply transformation (e.g., resampling, MFCC, MelSpectrogram)\n",
        "            if self.transform:\n",
        "                waveform = self.transform(waveform)\n",
        "\n",
        "            audio_tensors.append(waveform) # (1, F, T)\n",
        "            # audio_tensors.append(waveform) # (F, T)\n",
        "\n",
        "        # #Phoneme_files list of (1(audio_channels), F, T) tensors\n",
        "        #      -->  tensor (#Phoneme_files, 1(audio_channels), F, T)\n",
        "        audio_tensors = torch.stack(audio_tensors).to(dtype=torch.float32)\n",
        "\n",
        "        # Retrieve class label from metadata\n",
        "        # label = self.id_to_label[subject_id] - 1 # Label to index\n",
        "\n",
        "        return audio_tensors#, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b15ad1d-efe9-40d3-ad02-f3d3ea28f8ab",
      "metadata": {
        "id": "9b15ad1d-efe9-40d3-ad02-f3d3ea28f8ab"
      },
      "outputs": [],
      "source": [
        "# train transforms\n",
        "train_transforms = create_audio_transforms(augment=config['augument'])\n",
        "\n",
        "# val transforms\n",
        "val_transforms   = create_audio_transforms(augment=False)\n",
        "\n",
        "# test transforms\n",
        "test_transforms   = create_audio_transforms(augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba706de-6722-4374-938c-fe86cee25cb5",
      "metadata": {
        "id": "dba706de-6722-4374-938c-fe86cee25cb5"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "# subset function\n",
        "def subset_function(dataset, subset_selection):\n",
        "\n",
        "    # only use subset if its less than 1\n",
        "    if subset_selection < 1.0:\n",
        "\n",
        "        subset_size = int(len(dataset) * subset_selection)\n",
        "        indices = random.sample(range(len(dataset)), subset_size)\n",
        "        print(f\"Using subset of {subset_size}/{len(dataset)} samples ({subset_selection*100:.0f}%)\")\n",
        "        return Subset(dataset, indices)\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(f\"Using full dataset ({len(dataset)} samples)\")\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "pwI8-_t561_q"
      },
      "id": "pwI8-_t561_q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e49c2a-6c53-46f3-96b6-be9611054730",
      "metadata": {
        "id": "e6e49c2a-6c53-46f3-96b6-be9611054730"
      },
      "outputs": [],
      "source": [
        "train_dataset = AudioMultiPhonationDataset(\n",
        "    root_dir=config[\"train_dir\"],\n",
        "    phonation_types=phonation_list,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "train_dataset = subset_function(train_dataset, config['subset'])\n",
        "\n",
        "val_dataset = AudioMultiPhonationDataset(\n",
        "    root_dir=config[\"val_dir\"],\n",
        "    phonation_types=phonation_list,\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "val_dataset = subset_function(val_dataset, config['subset'])\n",
        "\n",
        "test_dataset = AudioMultiPhonationDataset_Test(\n",
        "    root_dir=config[\"test_dir\"],\n",
        "    phonation_types=phonation_list,\n",
        "    transform=test_transforms\n",
        ")\n",
        "\n",
        "test_dataset = subset_function(test_dataset, config['subset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanced Batch Sampler"
      ],
      "metadata": {
        "id": "LSv0KO40sDR2"
      },
      "id": "LSv0KO40sDR2"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# get labels for each id (0 indexed)\n",
        "train_labels = torch.tensor([train_dataset.id_to_label[id] - 1 for id in train_dataset.subject_ids])\n",
        "\n",
        "# get number of instances per class, create waits accordingly\n",
        "class_counts = torch.bincount(train_labels)\n",
        "class_weights = 1.0 / class_counts.float()\n",
        "\n",
        "# assign weights for each sample according to class weights\n",
        "sample_weights = class_weights[train_labels]\n",
        "\n",
        "# create sampler for train loader using our weights\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "STvAcaSrsLmc"
      },
      "id": "STvAcaSrsLmc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch.utils.data import Sampler\n",
        "\n",
        "# creating a balanced sampler so batches have roughly the same distribution\n",
        "\n",
        "class BalancedBatchSampler(Sampler):\n",
        "\n",
        "    def __init__(self, labels, batch_size=16, num_classes=5, max_batches_per_epoch=None):\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # group indices by class\n",
        "        self.class_to_indices = {c: [i for i, lbl in enumerate(labels) if lbl == c] for c in range(num_classes) }\n",
        "        self.class_pos = {c: 0 for c in range(num_classes)}\n",
        "\n",
        "        # per-batch target counts\n",
        "        self.base_count = batch_size // num_classes\n",
        "        self.remainder = batch_size % num_classes\n",
        "\n",
        "        # if max batches not provided, use batch size\n",
        "        natural_batches = len(labels) // batch_size\n",
        "        self.max_batches = max_batches_per_epoch if max_batches_per_epoch is not None else natural_batches\n",
        "\n",
        "\n",
        "    # iteration method for creating batches\n",
        "    def __iter__(self):\n",
        "\n",
        "        # iterate through the number of batches we specify\n",
        "        for _ in range(self.max_batches):\n",
        "\n",
        "            # add an extra sample if batch size isn't neatly divisible by number of classes\n",
        "            extra_classes = random.sample(range(self.num_classes), self.remainder)\n",
        "            batch_indices = []\n",
        "\n",
        "            # pick samples class by class\n",
        "            for c in range(self.num_classes):\n",
        "\n",
        "                count = self.base_count + (1 if c in extra_classes else 0)\n",
        "\n",
        "                for _ in range(count):\n",
        "\n",
        "                    # cycle through clasa samples\n",
        "                    idx = self.class_to_indices[c][self.class_pos[c] % len(self.class_to_indices[c])]\n",
        "                    batch_indices.append(idx)\n",
        "                    # track where we left off in cycle\n",
        "                    self.class_pos[c] += 1\n",
        "\n",
        "            # shuffle batch and return the indices\n",
        "            random.shuffle(batch_indices)\n",
        "            yield batch_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "X0NMSZVz7orl"
      },
      "id": "X0NMSZVz7orl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "id": "qBGe5Q-OsWIZ"
      },
      "id": "qBGe5Q-OsWIZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels from the train_dataset\n",
        "train_labels = [train_dataset.id_to_label[sid] - 1 for sid in train_dataset.subject_ids]\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "# 2) count appearances of each class\n",
        "class_sample_count = torch.tensor([(train_labels == t).sum() for t in torch.unique(train_labels, sorted=True)])\n",
        "\n",
        "# 3) inverse frequency = weight per class\n",
        "class_weights = 1.0 / class_sample_count.float()\n",
        "\n",
        "# 4) assign to each sample the weight of its class\n",
        "# if your labels are zero-indexed (0..4), this works as-is:\n",
        "sample_weights = class_weights[train_labels]\n",
        "\n",
        "# create sampler instance for training dataloader\n",
        "train_sampler = BalancedBatchSampler(train_labels, batch_size=config[\"batch_size\"], num_classes=5)\n",
        "\n",
        "# pass custom sampler to dataloader\n",
        "train_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True, batch_sampler=train_sampler)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False,  num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "Gd606WWZsXq3"
      },
      "id": "Gd606WWZsXq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect Distribution of Batches"
      ],
      "metadata": {
        "id": "o7qvLLk9oUv6"
      },
      "id": "o7qvLLk9oUv6"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# check the distributions of training batches and val batches\n",
        "def inspect_batches(dataloader, num_batches=5):\n",
        "    for i, (_, labels) in enumerate(dataloader):\n",
        "\n",
        "        # convert to list of labels\n",
        "        if labels.ndim > 1:\n",
        "            labels = labels.squeeze()\n",
        "\n",
        "        labels = labels.tolist()\n",
        "\n",
        "        # count and print occurances of classes in a batch\n",
        "        counts = Counter(labels)\n",
        "        sorted_counts = {cls: counts.get(cls, 0) for cls in sorted(counts.keys())}\n",
        "        print(f\"Batch {i+1}: {sorted_counts}\")\n",
        "\n",
        "        # stopping point for reporting batch distributions\n",
        "        if i + 1 == num_batches:\n",
        "            break\n",
        "\n",
        "# training batches should have roughly the same distribution\n",
        "print('training batches:')\n",
        "inspect_batches(train_loader, num_batches=5)\n",
        "\n",
        "# val batches should not have the same distribution\n",
        "print('\\nval batches')\n",
        "inspect_batches(val_loader, num_batches=5)\n"
      ],
      "metadata": {
        "id": "OTdPJbN7oUa3"
      },
      "id": "OTdPJbN7oUa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c4efe3b8-a2b1-41aa-9fe8-5139f4f248a8",
      "metadata": {
        "id": "c4efe3b8-a2b1-41aa-9fe8-5139f4f248a8"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d0c0a8-ac0b-44fe-8b8b-5829f4cc12f6",
      "metadata": {
        "id": "76d0c0a8-ac0b-44fe-8b8b-5829f4cc12f6"
      },
      "source": [
        "Show some samples, spectrograms, and listen to audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d509ac-1645-4ecb-b5b7-dd75305259e5",
      "metadata": {
        "scrolled": true,
        "id": "59d509ac-1645-4ecb-b5b7-dd75305259e5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(val_loader):\n",
        "    frames, labels = data\n",
        "    # print(f\"frames {frames.shape}\")\n",
        "    print(frames.shape, labels.shape)\n",
        "    print(labels)\n",
        "\n",
        "    # Visualize sample mfcc to inspect and verify everything is correctly done, especially augmentations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(frames[0][0][0].numpy().T, aspect='auto', origin='lower', cmap='viridis')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Features')\n",
        "    plt.title('Feature Representation')\n",
        "    plt.show()\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca937c8-d1b0-4fe9-ba43-97927f5f7d61",
      "metadata": {
        "id": "0ca937c8-d1b0-4fe9-ba43-97927f5f7d61"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ef9b85-a415-42d0-aa6f-ca890ab1cd05",
      "metadata": {
        "id": "d7ef9b85-a415-42d0-aa6f-ca890ab1cd05"
      },
      "outputs": [],
      "source": [
        "for i in range(len(train_dataset)):\n",
        "    x, y = train_dataset[i]\n",
        "    width = x.shape[-1]\n",
        "    break\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.vision_transformer import VisionTransformer, vit_b_16\n",
        "\n",
        "class ViTNetwork(nn.Module):\n",
        "    def __init__(self, num_classes, num_mels=64, time_steps=width,  # time_steps: width of spectrogram\n",
        "                 embed_dim=768, num_heads=12, depth=6, patch_size=16,\n",
        "                 aggregate=\"mean\"):  # 'mean' or 'lstm'\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: number of ALS categories to classify\n",
        "            num_mels: number of Mel bins (spectrogram height)\n",
        "            time_steps: number of time frames per spectrogram (width)\n",
        "            aggregate: how to combine multiple phonations ('mean' or 'lstm')\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Vision Transformer encoder ---\n",
        "        # self.vit = VisionTransformer(\n",
        "        #     image_size=(num_mels, time_steps),\n",
        "        #     patch_size=patch_size,\n",
        "        #     num_layers=depth,\n",
        "        #     num_heads=num_heads,\n",
        "        #     hidden_dim=embed_dim,\n",
        "        #     mlp_dim=embed_dim * 4,\n",
        "        #     num_classes=None  # We’ll take embeddings, not classification logits\n",
        "        # )\n",
        "        self.vit = timm.create_model(\n",
        "            \"vit_base_patch16_224\",\n",
        "            pretrained=False,\n",
        "            num_classes=0,\n",
        "            img_size=(num_mels, time_steps)\n",
        "        )\n",
        "\n",
        "        # --- Aggregation of multiple phonations (P) ---\n",
        "        self.aggregate = aggregate\n",
        "        if aggregate == \"lstm\":\n",
        "            self.lstm = nn.LSTM(input_size=embed_dim,\n",
        "                                hidden_size=embed_dim,\n",
        "                                num_layers=1,\n",
        "                                batch_first=True)\n",
        "        elif aggregate == \"mean\":\n",
        "            self.lstm = None\n",
        "\n",
        "        # --- Final classifier ---\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (B, P, 1, Mels, Time)\n",
        "           B = batch size, P = phonations per participant\n",
        "        \"\"\"\n",
        "        B, P, C, M, T = x.shape\n",
        "        x = x.view(B * P, C, M, T)\n",
        "\n",
        "        # ViT expects (B, 3, H, W); replicate channel if needed\n",
        "        if C == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "\n",
        "        feats = self.vit(x)  # (B*P, embed_dim)\n",
        "        feats = feats.view(B, P, -1)  # (B, P, embed_dim)\n",
        "\n",
        "        # Aggregate across phonations\n",
        "        if self.aggregate == \"mean\":\n",
        "            pooled = feats.mean(dim=1)\n",
        "        elif self.aggregate == \"lstm\":\n",
        "            _, (h_n, _) = self.lstm(feats)\n",
        "            pooled = h_n[-1]\n",
        "        else:\n",
        "            raise ValueError(\"aggregate must be 'mean' or 'lstm'\")\n",
        "\n",
        "        out = self.cls_layer(pooled)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b765b4-8a31-476d-91d4-32be6c970553",
      "metadata": {
        "id": "44b765b4-8a31-476d-91d4-32be6c970553"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "model = ViTNetwork(num_classes=config['num_classes']).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7322af56-f3d5-4e62-9dec-41a000421ecf",
      "metadata": {
        "id": "7322af56-f3d5-4e62-9dec-41a000421ecf"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2, 4, 1, 64, width).to(DEVICE)  # (B, P, 1, M, T)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(x)\n",
        "\n",
        "print(\"Logits shape:\", out.shape)      # Expected: (2, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4372e3a-f36b-4a11-879d-181cef23a967",
      "metadata": {
        "id": "c4372e3a-f36b-4a11-879d-181cef23a967"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-7)# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa095eb-65ce-4572-8e01-9da295377a03",
      "metadata": {
        "id": "caa095eb-65ce-4572-8e01-9da295377a03"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4a18dc-6d9b-4616-be5a-a6ea72bfa23f",
      "metadata": {
        "id": "2a4a18dc-6d9b-4616-be5a-a6ea72bfa23f"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (audio_tensors, labels) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        audio_tensors      = audio_tensors.to(DEVICE)\n",
        "        labels             = labels.to(DEVICE)\n",
        "\n",
        "        with torch.autocast(device_type=DEVICE, dtype=torch.float16):\n",
        "            ### Forward Propagation\n",
        "            logits  = model(audio_tensors)\n",
        "\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, labels)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == labels).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del audio_tensors, labels, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "from torchmetrics import F1Score\n",
        "\n",
        "f1 = F1Score(num_classes=5, average='macro', task='multiclass').to(DEVICE)\n"
      ],
      "metadata": {
        "id": "rwnorH9h4_yi"
      },
      "id": "rwnorH9h4_yi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1747e7a-09a0-4f88-aad4-ae961a907eb3",
      "metadata": {
        "id": "e1747e7a-09a0-4f88-aad4-ae961a907eb3"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (audio_tensors, labels) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device\n",
        "        audio_tensors      = audio_tensors.to(DEVICE)\n",
        "        labels             =  labels.to(DEVICE)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(audio_tensors)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, labels)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(preds == labels).item()/logits.shape[0]\n",
        "\n",
        "        # accumulate predictions and labels\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del audio_tensors, labels, logits, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    # Compute F1 scores of all predictions\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    vf1 = f1(all_preds, all_labels)\n",
        "\n",
        "    del all_preds, all_labels\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return vloss, vacc, vf1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d16b2992-0fa8-4e70-b537-092b02a2f2ac",
      "metadata": {
        "id": "d16b2992-0fa8-4e70-b537-092b02a2f2ac"
      },
      "source": [
        "## Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac89429c-c686-4c85-9f92-776c0bdd1469",
      "metadata": {
        "id": "ac89429c-c686-4c85-9f92-776c0bdd1469"
      },
      "outputs": [],
      "source": [
        "# add api key\n",
        "wandb.login(key=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f7a4ae-5a0f-4e2a-b2a5-f65026d5eb99",
      "metadata": {
        "id": "40f7a4ae-5a0f-4e2a-b2a5-f65026d5eb99"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "RESUME_OLD_RUN = False\n",
        "\n",
        "if RESUME_OLD_RUN == True:\n",
        "    print(\"Resuming previous WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        #id     = None,\n",
        "        resume = \"must\",\n",
        "        project = \"SAND\",\n",
        "        config  = config\n",
        "    )\n",
        "else:\n",
        "    print(\"Initializing new WanDB run...\")\n",
        "    run = wandb.init(\n",
        "        name    = f\"ablation {config['ablation_ID']}\",\n",
        "        reinit  = True,\n",
        "        project = None,\n",
        "        config  = config\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cc1e94-869d-41de-9e30-fe2ab01ce76d",
      "metadata": {
        "id": "25cc1e94-869d-41de-9e30-fe2ab01ce76d"
      },
      "outputs": [],
      "source": [
        "### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model)\n",
        "\n",
        "### Save it in a txt file\n",
        "arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "lTR65WzfxPTc"
      },
      "id": "lTR65WzfxPTc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27646eb-c7d3-4148-9b95-769e9bba50a8",
      "metadata": {
        "id": "a27646eb-c7d3-4148-9b95-769e9bba50a8"
      },
      "outputs": [],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "run_name = 'last_run'\n",
        "os.makedirs(run_name, exist_ok=True)\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc, val_f1      = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "    print(f'F1 Score: {val_f1}')\n",
        "\n",
        "    ## Log metrics at each epoch in your run\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr, 'valid_F1': val_f1})\n",
        "\n",
        "    # step scheduler using F1\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    # save model if f1 is best we've seen\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_f1': best_val_f1,\n",
        "            'train_acc': train_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        # Save checkpoint locally\n",
        "        checkpoint_path = f'best_model_epoch_{epoch+1}.pth'\n",
        "        full_path = os.path.join(run_name, checkpoint_path)\n",
        "        torch.save(checkpoint, full_path)\n",
        "\n",
        "        # # Save checkpoint to wandb\n",
        "\n",
        "        print(f\"Saved new best model with Val f1: {val_f1} at {full_path}\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57be9667-11ff-4946-93ca-f1de0960ce7f",
      "metadata": {
        "id": "57be9667-11ff-4946-93ca-f1de0960ce7f"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90c4395f-373b-4d58-9ead-334344a16398",
      "metadata": {
        "id": "90c4395f-373b-4d58-9ead-334344a16398"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.eval() # TODO train or eval?\n",
        "\n",
        "    all_preds = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch.no_grad(): # TODO\n",
        "\n",
        "        for i, audio_tensors in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            audio_tensors   = audio_tensors.to(DEVICE)\n",
        "\n",
        "            logits  = model(audio_tensors)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.append(preds)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "\n",
        "    ## SANITY CHECK\n",
        "    sample_predictions = all_preds[:10]\n",
        "\n",
        "    # Print a preview of predictions for manual inspection\n",
        "    print(\"\\n length of predictions:\", len(all_preds))\n",
        "    print(\"\\nPredictions Generated successfully!\")\n",
        "\n",
        "    return all_preds\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5202c1ef-8bf8-4c56-ae48-efc4563c930f",
      "metadata": {
        "id": "5202c1ef-8bf8-4c56-ae48-efc4563c930f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Generate model test predictions\n",
        "predictions = test(model, test_loader)\n",
        "\n",
        "all_labels = []\n",
        "for audio_tensors, labels in test_loader:\n",
        "    all_labels.append(labels)\n",
        "\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "print(f'length of ground truths: {len(all_labels)}')\n",
        "\n",
        "#from sklearn.metrics import f1_score\n",
        "#vf1 = f1_score(all_labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')  # or 'weighted' if class imbalance\n",
        "\n",
        "#print(f'f1 score on val dataset: {vf1}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e637e7-f6d7-4160-ba99-d86d9b2fba4a",
      "metadata": {
        "id": "24e637e7-f6d7-4160-ba99-d86d9b2fba4a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Create CSV file with predictions\n",
        "\n",
        "#with open(\"./submission.csv\", \"w+\") as f:\n",
        "#    f.write(\"id,label\\n\")\n",
        "#    for i in range(len(predictions)):\n",
        "#        f.write(\"{},{}\\n\".format(i, predictions[i]))\n",
        "#\n",
        "#    print(\"submission.csv file created successfully!\")\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}